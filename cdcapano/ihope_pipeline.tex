% ihope_pipeline.tex

% citation shortcuts
\def\12to18{Abbott:2009qj}
\def\sfive1yr{Collaboration:2009tt}
\def\sfivelvc{S5LowMassLV}


In this chapter we describe in detail the \ihope pipeline, which is the pipeline
used to carry out the \ac{CBC} search. This pipeline has been used to analyze
data taken from \ac{S5} \cite{\sfive1yr,\12to18}, \ac{S6}, \ac{VSR2}, and
\ac{VSR3} \cite{s6paper}.

\ihope can by modeled by a \ac{DAG}. A \ac{DAG} is a workflow in which the
output of one program, or {\it node}, is the input of another node or nodes,
such that the flow never loops back onto itself. It is represented by a diagram
in which the vertices are the programs and the edges show the interdependencies
\cite{condor}. \ihope is a \ac{DAG} of \ac{DAG}s: its nodes are workflows that
launch sub-workflows, which in-turn launch the programs that carry out the
analysis. These \ac{DAG}s are managed by the Condor High Throughput Computing
system, which distributes the jobs across the computer cluster and manages the
dependencies. Figure \ref{fig:ihopeOverview} shows an overview of the \ihope
workflow. Data from each \ac{IFO} is retrieved, analyzed in parallel workflows,
combined, and then output to a webpage. Each of these steps involves one or
more \ac{DAG}s.

In this chapter we go through each of these steps in detail. Section
\ref{sec:PipelineRequirements} reviews the requirements of a \ac{CBC} \ac{GW}
pipeline; section \ref{sec:ihopeRuntime} explains how the dag is set-up at
run-time; section \ref{sec:HIPEdetail} describes the HIPE pipeline in detail;
\ref{sec:TableStructure} describes the tables used to store data;
\ref{sec:PipedownDetail} describes Pipedown in detail and how the results are
presented. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=2.5in]{figures/ihopeOverview.pdf}
\end{center}
\caption{
An overview of the \ihope pipeline. The HIPE and Pipedown nodes are themselves workflows, and are detailed in later sectionss.
}
\label{fig:ihopeOverview}
\end{figure}

\section{Pipeline Requirements}
\label{sec:PipelineRequirements}

Before delving into the details of \ihope, we briefly review the key
requirements of a pipeline used to search for \ac{GW}s. Our goal is to search
for \ac{GW}s from coalescing binaries in a range of masses. Hampering our
efforts is the fact that gravitational waves couple very weakly to matter; we
must be able to detect strains of $\sim10^{-21}$. Despite this, we
aim to detect at a \ac{SNR} of $8$ in each detector to ensure statistical
confidence in our search.

When averaged over time the detectors' have a colored-Gaussian noise
distrubtion; as shown in Chapter \ref{ch:pipeline_principles}, the best analysis tool to search for signals is therefore a
matched filter \cite{?}. Match filtering requires knowing the morphology of the
waveform. Fortunately, binaries with total masses (\mtotal) less than $25\Msun$
sweep through the \ac{LIGO} and Virgo bands during their {\it inspiral} phases.
This means that the waveform from these systems can be well modelled by the
post-Newtonian approximation. For higher-mass systems ($25 < \mtotal/\Msun <
100$), in which the merger and ringdown part of the waveform become important,
numerical relativity waveforms can be stiched to the \ac{pN} approximation;
phenomenological waveforms may also be used. Thus, to cover the desired range,
we can fill a bank of templates using the methods described in section \ref{sec:multiple_templates} of Chapter \ref{ch:pipeline_principles}.

Environmental and instrumental factors can cause non-Gaussian transient noise
(\emph{glitches}) in the data. To deal with this our pipeline must be able to
distinguish between triggers resulting from glitches and triggers resulting
from gravitational waves. Since the morhpology of the signals is known, the
$\chi^2$ test discussed in Chapter \ref{ch:pipeline_principles} lends itself as
a good method to do this. Checking for coincident triggers in multiple
detectors will also filter out spurious triggers since we do not expect
environmental correlations across great distances. After all filtering and
tests have been applied, the statistical significance of a set of triggers has
to be evaluated to determine the probability that a gravitational wave exists
in the data. A pipeline must therefore be able to compute a background with
which to compare triggers to. To avoid bias, this must be done in a {\it blind}
fashion: the method by which background is chosen and triggers ranked must be
chosen without knowledge of what is in the data, else the results will sway in
the favor of the analysts' bias. 

Finally, the pipeline must be able to evaluate its sensitivity and efficiency
to sources in the universe. Doing so allows tuning studies to be carried out
prior to doing the full analysis, and for the astrophysical rate of \ac{CBC}s
to be bounded after the analysis has completed. This can be done by performing
\emph{injections} of signals with known parameters into the detectors. Both \emph{hardware} and \emph{software} injections may be performed. Hardware injections
involve actuating the mirrors to physically simulate a passing gravitational
wave. This is the most robust test as it checks the ability of the detectors'
response loop to measure \ac{GW} strains and the ability of the pipeline to
detect them. The trouble with hardware injections is that real \ac{GW}s cannot
be detected while they are occurring, limiting the number that can be done.
Software injections involve adding a gravitational wave signal to the data
stream on disk just prior to analyzing it. While this method doesn't test the
hardware control systems, it has the advantage that it can be done many times
in parallel, without corrupting the original data. Thus, our pipeline must be
able to perform software injections, and have a method for associating triggers
with the injections that went into the data.

In summary, a pipeline used to search for gravitational waves from \ac{CBC}s must:
\begin{itemize}
\item{construct a bank of templates with which to filter;}
\item{identify \emph{triggers} by filtering templates through the detector data;}
\item{distinguish noise triggers from gravitational wave triggers;}
\item{quantify statistical significance of triggers and rank them in a blind manner;}
\item{evaluate the sensitivity and efficiency of the search to \ac{CBC}s in the universe.}
\end{itemize}
In the following sections we will see how \ihope meets these requirements.


\section{\ihope at Runtime}
\label{sec:ihopeRuntime}

The \ihope pipeline is created by running \texttt{lalapps\_ihope}. This sets up
the workflow by doing the following at run time:

\begin{itemize}
\item{set-up the directory structure to save all data to;}
\item{copy all needed programs from their installed location to a local directory;}
\item{retrieve analysis start and stop times;}
\item{download a {\it veto-definer file} and find the start and stop times of all veto segments;}
\item{run \texttt{lalapps\_inspiral\_hipe};}
\item{run \texttt{lalapps\_cbc\_pipedown};}
\item{create a cache file of the names and locations of all files that will be created;}
\item{write a DAX that can be used to start and run the worflow.}
\end{itemize}

These steps require the start and stop time (in GPS seconds) of the period to
be analyzed as well as a configuration file. The configuration file is a text
file containing all the information needed to setup and run the analysis. This
includes: the names and locations of all the executables that will be run;
variable arguments that these programs will need; the name and number of
interferometers to analyze; the version of data files to retrieve and what
channels to analyze; how many and what type of software injection runs to do;
any other information needed by the \ac{DAG}s to run. The configuration file
provides a convenient way to manipulate the pipeline. Changing tuning
parameters is largely accomplished by editing this file. Likewise, the
difference between running a {\it low-mass} search ($2 < \mtotal/\Msun < 25$)
and a {\it high-mass} search ($25 < \mtotal/\Msun < 100$) is determined
entirely by the configuration file.

A directory named by the gps start/stop times is created at runtime. All work
is done in this directory. In it, a \texttt{segments}, \texttt{executables},
\texttt{datafind}, \texttt{full\_data}, and \texttt{pipedown} directory are
created, along with a directory for each injection run that will be carried
out. With the exception of the \texttt{executables} and \texttt{segments}
directory, each of the sub directories store a sub-\ac{DAG} that will be run
during the analysis (and will be explained below). The master \ac{DAG} is saved
in the gps-times direcotory along with a master cache file of all the files
that will be created. All programs that will be run are copied to the
\texttt{executables} directory.

\subsection{Science and Veto Segments Retrieval}
\label{sec:science_segs_and_vetoes}

The \ac{LIGO} and Virgo detectors can be in one of five different states at any
given time. We are only interested in analyzing times in which the detectors
are in {\it Science} mode. This means they are up, locked, and no other
experimental work is being done on them \cite{?}. The interferometers
can drop out of Science mode many times across an analysis period; thus \ihope
must retrieve the start and stop times of Science segments that occurred in the
desired analysis period. \ihope does this by running
\texttt{ligolw\_segment\_query} at run time. This program queries the {\it
segment database} --- a remote database that contains lists of segments
detailing the times that each of the detectors were in various states --- to
retrieve the list of Science times during the desired analysis period. These
results are saved to xml files in the \texttt{segments} directory. These files
do not contain strain data; they only list the times that data can be
retrieved. The results are later used to retrieve files containing strain data.

Various environmental and instrumental factors can cause periods of elevated
glitch rate during Science mode. If these periods are analyzed with periods of
relatively clean data, they will pollute the background estimation, thereby
decreasing statistical confidence in candidates. We therefore seek to remove
such periods from the analysis. This is accomplished using vetoes. Vetoes are
categorized according to how well we can couple them to known environmental
sources; they are applied cumulatively. Table \ref{tab:vetocats} lists the
various categories and their defining characteristics. For \ac{CBC} searches,
we do not analyze anything prior to category 1; i.e., all matched filtering is
done after category 1 vetoes are applied. Category 2 and 3 vetoes are applied
when second stage coincidence is carried out (see HIPE, below). We quote false
alarm rates and base upper limits on data in which category 1-3 vetoes have
been applied. We additionally check the data after category 1 and 2 vetoes have
been applied for any loud triggers that may have been removed by category 3
vetoes. We do not use category 4 for the anlaysis; however, we do use them in
follow-up studies of loud events to provide insight into the cause of the
events. Hardware injections are left in the data after category 1 and 2, and
are removed as a special veto prior to category 3 vetoes being applied.

\begin{table}
\label{tab:veto_cats}
\center
\begin{tabular}{c | p{5cm} | p{8cm}}
Category    &    Description    &   Procedure    \\
\hline
    1       &    Data seriously compromised or missing.    &    Data never analyzed. \\
\hline
    2       &    Instrumental problems with known coupling to h(t).    &    Vetoed triggers discarded after second coincidence. Surviving triggers checked for candidates, but not used for upper limits. \\
\hline
    3       &    Instrumental problems likely, casting doubt on triggers found during these times.    &    Vetoed triggers discarded after second coincidence. False alarm rates of surviving triggers are used in publications; upper limits are calculated using these vetoes.  \\
\hline
    4       &    Positive, but weak, correlations with false alarms. Large dead times.    &     Not used in the analysis, but used as a guide in detailed followups of loud triggers. \\
\end{tabular}
\caption{The various veto categories used by the \ac{CBC} group. Vetoes are applied cumulatively; statistical significance of candidates and upper limits are calculated after category 1, 2, and 3 vetoes are applied.}
\end{table}

Vetoes are triggered by environmental and instrumental channels that flag
various segments of time for suspicious activity. Additional flags can be added
by hand; e.g., if a truck drives onto the site during Science mode, a person
in the control room may add a flag for that period of time. All of these flags
are stored in the segment database. What flags to use for vetoes, at what
category, and for how long, are stored in a \emph{veto-definer file}. This xml
file contains a \texttt{veto\_definer} table that lists each flag that should
be used, what category the flag should be used at, the dates the flag is valid, and any
padding (in seconds) to add to the flag should it go off. Entries are added to
this table by hand after extensive data-quality investigations and safety
checks. Vetoes are fine tuned for specific searches and epochs, and each
searches' set is saved in a different veto-definer file in a central
repository. What veto definer file to use is specified in the \ihope
configuration file. At run-time, \ihope downloads the desired file to the
\texttt{segments} directory. It then runs \texttt{ligolw\_segments\_from\_cats}
to query the segment database for flags specified in the veto-definer file. The
vetoed segments for all of the instruments are added together and saved in xml
files in the \texttt{segments} directory.


\subsection{HIPE}
\label{sec:hipe_overview}

Once the analyzable Science segments and the veto segments that will be applied
are obtained, \ihope runs \texttt{lalapps\_inspiral\_hipe}. This sets up the
\ac{HIPE}, which is the pipeline that carries out the search. Figure
\ref{fig:HIPEDiagram} shows the \ac{HIPE} \ac{DAG} for a single $2048\rm{s}$
block of time. As can be seen in the diagram, \ac{HIPE} is a \emph{two-stage}
pipeline. Data is matched-filtered, coincidence tests are applied, then the
process is repeated. The reason for using two-stages is the $\chi^2$ test is
computationally expensive. Therefore, to cut down on the number of triggers for
which we need to compute $\chi^2$, an initial coincidence test is
applied.\footnote{Using two stages complicates the pipeline, however, and makes
it difficult to trace an event's progression through the various steps. It also
hampers efforts to estimate \acp{FAR} from single-\ac{IFO} triggers. For this
reason, a single-stage pipeline is being worked which takes advantage of
advances in computational power. See Chapter \ref{ch:future_developments} for
more details.}

\ac{HIPE} can be run both with and without injections. If injections are
desired, \texttt{lalapps\_inspinj} is run to create a list of injections to
make, which are created and inserted into the data just prior to match
filtering by \texttt{lalapps\_inspiral}. In order to create data with and
without injections, \ihope runs \ac{HIPE} several times: once for zero-lag and
time-slid data (which we label \texttt{FULL\_DATA}), and once for each desired
injection run, which are specified in the configuration file.

As discussed in section \ref{sec:pipeline_requirements}, we must be able to
estimate a background and calculate triggers in a blind fashion. However, we
must also be able to tune the pipeline so as to improve the chances of
detecting something. To satisfy these conflicting requirements, we have
designated a subset of data as \emph{playground}. Playground is defined as data
that occurs during the first $600$ seconds of every $6370\th$-second block of
time, starting from February 14, 2003 at 16:00:00 UTC (GPS time $729273613$).
It therefore consists of $\sim10\%$ of the full data. Playground is looked at
prior to un-blinding the rest of the data (what we colliquoly call
\emph{opening the box}) in order to tune vetoes and check for any spurious
results that would indicate a bug in the analysis. Although playground is
included in the final open-box analysis, we exculde it for computing
upper-limits. It is possible to run \ac{HIPE} such that it only analyzes
playground data; however, it is unnecessary, as we can easily retrieve
playground triggers from the \texttt{FULL\_DATA} analysis using their
end-times. Details of \ac{HIPE} are discussed in section \ref{sec:HIPEdetail}.

\begin{figure}[p]
\begin{center}
\includegraphics[width=6in]{figures/HIPEDiagram.pdf}
\end{center}
\caption{
The \ac{HIPE} pipeline. This is run once for full-data and once for
each injection run. For injection runs, \texttt{lalapps\_inspinj} is run to
generate a list of injections to create, and time slides are not done.
}
\label{fig:HIPEDiagram}
\end{figure}

\subsection{Pipedown}
\label{sec:pipedown_overview}

After all the instances of \texttt{lalapps\_inspiral\_hipe} have run, \ihope
run \texttt{lalapps\_cbc\_pipedown} which sets up the Pipedown \ac{DAG}.
Pipedown takes the results of all the different HIPE runs, combines them into
SQLite databases, computes and ranks triggers by \ac{FAR}, and creates plots
and tables of the results. Figure \ref{fig:PipedownDiagram} details the steps
Pipedown takes to carry out these goals. Shown are the steps taken for a single
veto-category; this diagram is repated for each veto-category (by Pipedown, not
by \ihope). In-depth details of Pipedown are discussed in Section
\ref{sec:PipedownDetail}.

\begin{figure}[p]
\begin{center}
\includegraphics[width=6in]{figures/PipedownDiagram.pdf}
\end{center}
\caption{
The Pipedown pipeline for a single veto category. Each block represents a single node. Double bordered blocks represent multiple nodes. Circles represent batches of files. Black arrows represent xml files; blue arrows, SQLite databases. Each arrow represents a single file.
}
\label{fig:PipedownDiagram}
\end{figure}

\subsection{DAX}
\label{sec:DAX}

After pipedown has completed, \ihope writes a DAX that can be used to launch the pipeline. A DAX is an abstract workflow in which elements such as file locations are variables. The DAX is turned into a \ac{DAG} by the Pegasus Workflow Management Service.

\section{HIPE in Detail}
\label{sec:HIPEdetail}

We now step through \ihope in detail, using a toy analysis of $10,240$s as an example. In this analysis we will use three interferometers: the 4-kilometer Hanford detector (H1), the 4-kilometer Livingston detector (L1), and the 3-kilometer Virgo detector (V1), and we will add one injection run, which we label \texttt{BNSINJ}. Figure \ref{fig:science-selected_segs} shows the segments that are available to analyze during this time. The \emph{selected segments} are Science segments minus CAT1 veto segments; they are what \ihope passes to \ac{HIPE} to analyze.

\subsection{Data Find}
\label{sec:data_find}

As can be seen in Figure \ref{fig:HIPEDiagram}, the first step in \ac{HIPE} is to run \texttt{lalapps\_data\_find}. Data from all of the interferometers are stored in \emph{frame files} in central locations at every computer cluster. Frame files can contain multiple channels recorded from the interferometers. For analysis purposes, we are only interested in files containing the strain data channel, which is called \texttt{LDAS-STRAIN} in the \ac{LIGO} detectors and \texttt{h\_16384Hz} in Virgo. \texttt{lalapps\_data\_find} finds the files covering the selected segments on the cluster and creates cache files listing the location of each these files in the \texttt{datafind} directory. These cache files are passed to \texttt{lalapps\_tmpltbank} and \texttt{lalapps\_inspiral}, which use them to locate and open the frame files for analysis.

\subsection{From Continuous to Discreet Data}
\label{sec:cont_to_discreet}

So far, all equations we have used involving Fourier Transforms and the inner product have assumed continuous time- and frequency-domain data series. Likewise, the integrals over frequency space have been from $-\infty$ to $+\infty$. In practice, of course, the data is discreetly sampled and is neither continuous nor infinite. Both the \ac{LIGO} and Virgo strain data are sampled at $16384\,\mathrm{Hz}$. Since even the lowest mass templates (i.e., waveforms with the highest frequency components) used in current \ac{CBC} searches terminate at frequencies around a couple of kHz, this sampling rate is much higher than is needed for our purposes. To ease computational requirements the time series is therefore downsampled to $4096\,\mathrm{Hz}$ prior to analysis. This sampling rate sets the Nyquist frequency, $f_{\mathrm{Nyquist}}$, at $2048\,\mathrm{Hz}$. To prevent aliasing, a low-pass time-domain digital filter with a cutoff at $f_{\mathrm{Nyquist}}$ is implemented to pre-condition the data. On the low-frequency end, seismic noise dominates the interferometers' power spectrum. We therefore also impose a high-pass digital filter in the time domain. The cutoff frequency of the high-pass filter, $f_c$, is set to be a several Hz lower than a low-frequency cutoff, $f_0$, that is determined by the characteristics of each \ac{IFO}'s power spectrum. These values are set in the configuration file. (For the values chosen for \ac{S5} and \ac{S6} see Chapters \ref{ch:s5_results} and \ref{ch:s6_results}, respectively.) Both the low and high-pass filters will ring at the start and end of a time series, corrupting the data. Thus we must remove the first and last $t_{\mathrm{pad}}$ of data after applying the filters and prior to analyzing. The duration of $t_{\mathrm{pad}}$ is also set in the configuration file; for both \ac{S5} and \ac{S6} we used $8\,$s.

To Fourier Transform the data, we use the \ac{FFT} algorithm. The \ac{FFT} imposes two constraints on the data. First, the number of points in the data series must be a power of two. Second, the \ac{FFT} associates the last point of the data series with the first point; i.e., it wraps the data around on a loop. This means that as a template is slid toward the end of the time series, any points extending beyond the end of the series will be placed at the beginning. Additionally, because the last and first point of the time series are discontinuous, the wrap around essentially introduces a delta function at the end of the series. As the template passes past the discontinuity it will ring (i.e., the match filter will return the template's impulse response), and, due to the wrap around, this ringing will be placed at the beginning of the time series. Thus the first $t_{\mathrm{chirp}}$ points of the \ac{SNR} time series are corrupted, where $t_{\mathrm{chirp}}$ is the \emph{chirp length} of the template, and must be thrown out.

The chirp length is defined as the length of time it takes for the binary to go from $f_0$ to the frequency at which the binary passes \ac{ISCO}, $f_{\mathrm{isco}}$. \ac{ISCO} is the point when the separation distance is too small for the binary's masses to maintain stable orbits without external energy being added to the system; at this point the masses cease to inspiral and plunge into each other.\footnote{Note that \ac{ISCO} is not the same as the Schwarzschild radius; e.g., in a Schwarzschild space-time, \ac{ISCO} occurs at $r = 6GM/c^2$ whereas the Schwarzschild radius occurs as $r = 2GM/c^2$.} Since the \ac{pN} approximation breaks down at $f_{\mathrm{isco}}$, we must terminate all integrals at this point. Combining this with the realities of Nyquist and seismic noise, all frequency domain integrals are limited to the region $f \in [f_0, f_{\mathrm{max}})$ where $f_{\mathrm{max}} = \min(f_{\mathrm{Nyquist}},~f_{\mathrm{isco}})$.

The \ac{PSD} is estimated using Welch's method. This involves breaking a data segment up into several bins of equal duration. Within each bin, the data is transformed to the frequency domain via the \ac{FFT}. Thus the number of points in each bin must be a power of two, and the bins must be overlapping to account for the wrap-around corruption at the beginning and end of each segment. Since the data is discreet, this results in a discreet number of frequency bins. The \emph{median} value within each frequency bin is then chosen across all the data segments to construct $S_n(|f|)$. We use the medain --- as opposed to the mean --- to better buffer the \ac{PSD} from over-estimation in the prescence of a signal.

As was the case with the template, $S_n(|f|)$ will be corrupted by the discontinuity between the first and last point of the data series. However, unlike the template, which has a finite duration, $S_n(|f|)$ will ring for the entire data segment. This ringing will also happen if a delta-function like glitch exists in the data. To prevent the entire segment from being corrupted, we \emph{truncate} the \ac{PSD}. This is done by estimating \ac{PSD} using Welch's method, inverse transforming $\sqrt{S_n^{-1}(|f|)}$ to the time domain, zeroing out the first and last $t_{\mathrm{invspectrunc}}$ seconds of the time series, then transforming back to the frequency domain. This limits the corruption due to the wraparound to the first and last $t_{\mathrm{invspectrunc}}$ seconds of the time series. The truncation does cause some smoothing out of high Q features, such as power-line harmonics; however, since we search for relatively broad band signals this smoothing has little effect on the search. The value of $t_{\mathrm{invspectrunc}}$ was set to $8\,$s for both \ac{S5} and \ac{S6}.

For more details on \ac{PSD} estimation and implementation of the matched filter with a discreet and finite time-series, see \cite{ref:Brown} and \cite{ref:FindChirp}.

\subsection{Data Segmentation}
\label{sec:data_segmentation}

\texttt{lalapps\_inspiral} is the program that constructs the \ac{SNR} time series for templates laid out in a bank by the program \texttt{lalapps\_tmpltbank}. The \ac{SNR} time series is constructed according to equation \ref{eqn:snr_full_form} and the templates are laid out using the metric defined in equation \ref{eqn:templateMetric}. Since both of these equations involve the inner product defined in equation \ref{eqn:inner_product}, both programs must compute the \ac{PSD}, $S_n(|f|)$, as well as the Fourier Transform of the data, $\widetilde{s}(f)$. (We do not Fourier Transform the templates. Instead, we generate the templates in the frequency domain directly by using the \ac{SPA}.)

The wrap around of the \ac{FFT} and the Welch median \ac{PSD} estimation method put limitations on how much data can be analyzed at once; they also require data to be overlapped due to corruption at the start and end of segments. These considerations require us to carefully segment the data for analysis. The things we must consider are:
\begin{itemize}
\item{The low-pass and high-pass digital filters corrupt the first $8\,$s of an analysis segment, or \emph{chunk}.}
\item{In order to estimate the \ac{PSD} using the Welch median method, we must break the chunk up into several segments; data in each segment is transformed via the \ac{FFT} independently.}
\item{The \ac{FFT} requires the number of points in each segment to be a power of two.}
\item{The wrap around of the \ac{FFT} causes the first $t_{\mathrm{chirp}}$ seconds of each segment to be corrupted due to the template, and the first and last $8\,$s of the segment to be corrupted due to the inverse \ac{PSD}.}
\item{Each segment must therefore overlap so as not to lose data from the corrupted parts of the \ac{FFT}.}
\end{itemize}
In initial \ac{LIGO} the low-frequency cutoff $f_0$ was set to $40\,$Hz. In the low-mass \ac{CBC} search, the longest template is therefore $\sim33\,$s in duration. The smallest power of two larger than $33$ is $64$; thus the first $64\,$s of each segment must be overlapped by a previous segment. The end of each segment only needs to be overlapped by $8\,$s to account for the \ac{PSD} corruption. However, for book-keeping simplicity, we chose to also discard the last $64\,$s of each segment. Thus, all the segments must overlap each other by $128\,$s. Since the segments must be a power of two, this means each segment must be $256\,$s in duration.

In deciding upon the number of segments to use in the median \ac{PSD} estimate we must consider a few factors. The more segments we have, the more accurate the \ac{PSD} estimation will be. However, more segments also means that more time will be grouped together. If the \ac{PSD} changes over this period, our estimate will be off. Furthermore, the number of segments used sets a minimum period of time that we must have continuous data. If a selected segment is shorter than that period, we cannot analyze it. 

In inital \ac{LIGO} we have chosen to use 15 overlapping segments for each \ac{PSD} estimation. Since each segment is $256\,$s long, with overlaps of $128\,$s, this means that each chunk must be $2048\,$s long. Add to this the needed $8\,$s pad at the start and end of the chunk to account for the corruption of the filters, and we find that we need a continous $2064\,$s of data in order to do the analysis. If a selected segment is longer than $2064\,$s, then it will be broken up into several chunks. Each chunk is overlapped by $72\,$s to account for the time thrown out at the beginning/end of the first/last segment in each chunk and the $8\,$s pad. If the selected segment is not a multiple of $2048$ (as it most likely will not be), then the last chunk in the segment is overlapped more so as to analyze the entire period. The segments in this last chunk that overlap with non-corrupted time in the previous chunk are simply not match filtered, although they are used for the \ac{PSD} estimation of the last chunk.

The effects of this data segmentation on our toy analysis can be seen in Figure \ref{fig:segment_plot_full}. The top three lines show the selected segments. Each \texttt{TMPLTBANK} and \texttt{INSPIRAL} jobs corresponds to a single chunk in a single \ac{IFO}. As can be seen, every chunk overlaps by $72\,s$, except for the last one in each segment. Also note the first selected segment in V1. As seen in Figure \ref{fig:science-selected_segs}, a CAT 1 veto broke the V1 Science segment into two selected segments. Since the first selected segment is only $\sim900\,$s long, it cannot be analyzed. This can be seen in the \texttt{TMPLTBANK} and \texttt{INSPIRAL} lines: there are no V1 jobs covering this period. This is part of the reason why CAT 1 vetoes are only used for seriously compromised data. Overuse of CAT 1 vetoes could lead to large amounts of unanalyzed time if they broke the data up into selected segments that were shorter than $2064\,$s.

\subsection{Template Bank}
\label{sec:tmpltbank}

The first step in the \ac{HIPE} pipeline after data find is to create a template bank. As stated above, \texttt{lalapps\_tmpltbank} is the program that constructs the bank. \ac{HIPE} creates one \texttt{tmpltbank} job for each \ac{IFO} and for each chunk. Since the \ac{PSD} is re-estimated for each chunk the metric will change from job-to-job, resulting in a different template bank for each chunk and for each \ac{IFO}. However, the changes in the \ac{PSD} are usually small, and so the bank stays roughly the same across an analysis period. In order to calculate the \ac{PSD} \texttt{lalapps\_tmpltbank} reads in the frame cache, loads the data, applies the low- and high-pass filters, downsamples to $4096\,$Hz, then computes the \ac{PSD} using the Welch median method.

Variables such as what \ac{pN} order to use to lay out templates, what space to lay them out in, and what minimal match to use are command-line arguments to \texttt{lalapps\_tmpltbank} and can therefore be set in the configureation file. As stated in Chapter \ref{ch:pipeline_principles}, for \ac{S5} and \ac{S6} the bank was laid out in $\tau_0$ and $\tau_3$ space using $2.0$ restircted \ac{pN} templates to calculate the metric, with a minimal match $\geq 97\%$. The \ac{pN} order of the templates themselves do not have to be the same as the order used to compute the metric. Indeed, while $2\,$\ac{pN} templates were used for \ac{S5}, for \ac{S6} restricted $3.5\,$\ac{pN} templates were used. Since the metric and best coordinates to use is currently unknown for $3.5\,$\ac{pN} templates, we had to use the $2\,$\ac{pN} metric. Investigations are underway into obtaining the $3.5\,$\ac{pN} metric.

Figure \ref{fig:template_bank} shows a typical template bank in both $\tau_0,~\tau_3$ space and in $m_1,m_2$ space for the low-mass \ac{CBC} search.\footnote{Note that the maximum total mass in these plots extends to $35\,\Msun$. This was reduced to $25\,\Msun$ for the last part of \ac{S6}. See Chapter \ref{ch:s6_results} for details.} The parameters of each of these templates are stored in a \texttt{sngl\_inspiral} table (see section \ref{sec:data_storage} for details), which is saved in an xml file. Also stored in the \texttt{sngl\_inspiral} table are the metric components around each template. Each \texttt{tmpltbank} job outputs one xml file, with naming convention:
\begin{center}
\texttt{\{IFO\}-TMPLTBANK-\{GPS-START\}-\{DURATION\}.xml}
\end{center}
All of the \texttt{tmpltbank} files created in our toy $10240\,$s analysis can be seen in Figure \ref{fig:segment_plot_full}.

\subsection{Injections}
\label{sec:inspinj}

Software injections are the \ac{CBC} group's way to check that our pipeline is working.\footnote{As mentioned in section \ref{sec:PipelineRequirements} hardware injections exist too. These are also used to check that the pipeline is working. However, because we cannot perform the large number of hardware injections needed for Monte Carlo simulations --- as we can with software injections --- they are of limited use for tuning and efficiency studies.} Since the number we can perform is only limited by computational power, we can do a large number across a broad range of parameters and sky positions. Being able to do such Monte Carlo simulations allows us to test the efficiency of our pipeline, tune parameters, and calculate rate limits to compare to astrophysical expected rates.

Injections are only performed in injection \ac{HIPE} runs; these are later combined with non-injection runs by Pipedown. For example, in our toy analysis, we have chosen to do one injection run, which we labeled \texttt{BNSINJ}. If we are on an injection run then, prior to launching first inspiral jobs, \ac{HIPE} runs \texttt{lalappps\_inspinj}. \texttt{Inspinj} generates a list of injections to perform based on the input arguments. It does not create the waveforms themselves; that is left to \texttt{lalapps\_inspiral} (see below).

How the injections are distributed in mass, mass-ratio, sky-location, orientation, and time is determined by the configuration file. For a given injection run, a minimum and maximum component mass are specified, along with a maximum total mass. Additionaly, what mass parameter to distribute the injections in must be specified. For \ac{S5} and \ac{S6} injections were chosen to be distributed uniformly in component mass. How to distribute the location of injections is also required. In \ac{S5} a galaxy catalogue was used for the location distribution; random galaxies were chosen for an injection to occur in. For \ac{S6}, the range of the detectors was large enough to ignore inhomogenities in the universe; thus in \ac{S6} the location distribution was changed to be randomly distributed across the sky. The orientation of injections was distributed uniformly across inclination angles for both \ac{S5} and \ac{S6}.

Injections are distributed randomly in time. However, to prevent too many injections from occuring in the same period, a time-step and time-interval argument are added. The time-step argument sets the average period of time between each injeciton, and the time-interval sets the interval around that time-step in which an injection is randomly placed. For example, in \ac{S6} the time-step was set to $837\,$s and the time-interval was set to $300\,$s. This means that if an injection occurs at time $t_0$, the next injection will be chosen to randomly occur in the interval $(t_0 + 837) \pm 300\,$s.

We distribute injections in distance based on the range of the detectors. We are most interested in the region around the network's horizon distance, as this is where the efficiency quickly drops from $\sim1$ to $0$ (for a given false alarm rate). Since the detectors' sensitivity is mass dependent, we adjust the domain of distances in which to distribute injections according to the mass-range and type of injections being performed. Injections may be distributed uniformly in distance or in log distance. When we distribute uniformly in distance, we tend to over populate the outer regions of the range, since volume grows as $r^3$. If we distribute uniformly in log distance, we tend to over populate the inner regions of the range. For this reason, two injection sets are typically done for a single mass-range and injection type: one that is distributed linearly in distance and one distributed uniformly in log distance. Prior to analysis, the choice of injection ranges is based on a best-guess of what the range will be from \ac{PSD} estimations. Since we cannot know what the actual range of the network of detectors is until after the analysis is complete, extra injection runs are done after the analysis. These runs are set up to better target the range around the horizon distance so as to have good statistics for upper-limit calculations.

The waveform generator used by \texttt{lalapps\_inspiral} --- called \texttt{FindChirpSP} --- has the ability to create waveforms from several different template families, not just restricted non-spinning \ac{pN} templates. Thus we check how well spinning waveforms are recovered using our non-spinning template bank. This was done for both \ac{S5} and \ac{S6} searches, as separate upper-limits were produced for spinning \ac{NSBH} and \ac{BBH} systems. Additionally, we can check how well various waveform families overlap with the \ac{pN} approximation we use for the templates in the template bank. For example, in the \ac{S6} low-mass \ac{CBC} search, \ac{NSBH} injections were generated using the EOBNR psuedo-4\ac{pN} model.

Each injection \ac{HIPE} run will only draw from a single waveform family, with a give range of parameters, and with a given random-number-generator seed. To create a large number of injections, multiple injection runs are done; some of these will have the same range of parameter, with only the random seed changing. When \texttt{lalapps\_inspinj} runs in a given injection run, it saves the list of injections to perform to a \texttt{sim\_inspiral} table that lists the times, parameters, and waveform family of the injections to create. This table is saved to an xml file with naming convention:
\begin{center}
\texttt{HL-INJECTIONS\_\{SEED\}\_\{USER-TAG\}-\{GPS-START\}-\{DURATION\}.xml}\footnote{The \texttt{HL} prefix on the \texttt{INJECTIONS} file is an anachronism from when searches were only done between the two \ac{LIGO} sites. The \texttt{sim\_inspiral} table actually contains injection information for all the detectors in the search.}
\end{center}
\texttt{SEED} is the random seed given to \texttt{inspinj} in the given run. The \texttt{USER-TAG} is a unique identifier given to each \ac{HIPE} run by \ihope. The non-injection run will be named \texttt{FULL\_DATA} whereas each injection run will have a unique name to distinguish it from the other injection runs. Unlike the \texttt{FULL\_DATA} tag, the injections' \texttt{USER-TAG}s are set in the configuration file. For example, since we have done one injection run in our toy analysis, we have two unique user-tags: \texttt{FULL\_DATA} and \texttt{BNSINJ}. One \texttt{INJECTIONS} file exists for each injection run; this file is loaded by \texttt{lalapps\_inspiral} to insert the injections into the data.


\subsection{First Inspiral}
\label{sec:first_inspiral}

With the template bank and (for injection runs) list of injections generated, \ac{HIPE} next runs \texttt{lalapps\_inspiral} to match filter the data. One inspiral job exists for each \texttt{tmpltbank} job; i.e., there is an inspiral job for each \ac{IFO} and for each analysis chunk. Inspiral loads the frame cache and the template bank corresponding to its chunk. Additionally, if \ac{HIPE} is analyzing an injection run, it will read in an \texttt{INJECTIONS} file.

Inspiral carries out the same data conditioning as \texttt{tmpltbank} (in fact, they call the same code): data is read in, low- and high-pass filtered, and resampled. If \texttt{inspiral} is given an injections file it will create the waveforms that overlap its analysis time prior to data conditioning. Waveforms are generated in the time-domain using the parameters stored in the \texttt{sim\_inspiral} table and added directly to the data stream in memory. The data is then low- and high-pass filtered and resampled. Note that \texttt{tmpltbank} does not load injections. Instead, the same bank is used for both non-injection and injection runs. While this introduces a subtle difference from the real situaton, the effect on the template bank is negligible since the median estimator is used and since the number of injections in a chunk is limited by the time-step and time-interval arguments given to \texttt{lalapps\_inspinj}.

After the data is conditioned and the \ac{PSD} estimated, \texttt{inspiral} reads in a template from the \texttt{TMPLTBANK} file, generates it (in the frequency domain), then match filters it with the data to create the (complex) \ac{SNR} time series for the given template. This is repeated for each template in the bank file. For injection runs, an optional argument \texttt{enable-inj-filter-only} can be turned out that causes the \texttt{inspiral} to only filter segments containing an injection. This cuts down on computational time and has no effect on the analysis, since we are only interested in times around an injection (all other times are assumed to return the same result as the non-injection run).

As discussed in Chapter \ref{ch:pipeline_principles} we identify triggers by finding points where the \ac{SNR} ($\rho$) is at a maximum. Due to noise, however, there will be multiple local maxima across the duration of a template. We must therefore employ time-domain clustering on the \ac{SNR} so as to associate a single trigger with an event. This is done by using the \emph{max over chirp length} algorithm. Max over chirp length uses a sliding time window to select triggers: for every point in time, a point is only kept if there is no other point with a $\rho$ greater than it within the chirp length of the template. Once a trigger is identified, the time at which it occurs is associated with the \emph{coalescence}, or \emph{end-time}, $t_c$, of the binary. If the \ac{SNR} of the trigger exceeds our desired \ac{SNR} threshold, it is kept. The \ac{SNR} threshold is set in the configuration file; for both \ac{S5} and \ac{S6} it was set to 5.5.

Due to the high overlap between templates in the bank, a single event can create triggers across multiple templates. In order to associate a single trigger with a single event, we also need to cluster across the bank. \texttt{lalapps\_inspiral} offers two options: hard-window clustering, and \emph{trigscan}. The hard-window clustering is the simplest of the two: the time-series is split up into windows with a set duration (determined in the configuration file). Within each window, the trigger with the largest \ac{SNR} is kept while all others are discarded. This method has the advantage of being simple, and it garauntees that the trigger rate in a single \ac{IFO} will not exceed one per the window duration. However, choosing the size of the window is difficult and somewhat arbitrary. Different templates have varying impulse responses and will ring for varying amounts of time depending on the strength of a signal or glitch. Thus the window is unlikely to cause only one trigger to be associated with one event. Additionally, a glitch that rings off some triggers at one end of the bank can cluster away a decent signal candidate that rang off some templates at the other end of the bank, even though the glitch and the signal candidate look nothing like each other.

A more sophistcated approach is to use trigscan clustering \cite{ref:Keppel}. Rather than simply use the time-dimension, trigscan also pulls in information about the parameters of the templates to cluster. The method is similar to that of coincidence testing: for a given trigger, trigscan constructs an error ellipse with size $\epsilon_{ts}$ around the trigger using the bank metric computed by \texttt{tmpltbank}. (The value of $\epsilon_{ts}$ is determined from tuning studies and is set in the configuration file).) It then collects all triggers that fall within that ellipse. Ellipses are then constructed around each new trigger found, and more triggers are collected around them. This continues until no more triggers can be found within any ellipse. The trigger with the largest \ac{SNR} amongst the collected triggers is then kept. By involving parameter information this method has the advantage that it can cluster on a good candidate on one end of the bank without being affected by a glitch on the other end of the bank. Also, since time is incorporated in the construction of the error ellipse, the time window will adjust for each template and for the \ac{SNR} of the triggers. The disadvantage to this method is that it does not gaurauntee a maximum trigger rate. (This proved to be a problem for \ac{S6}; see Chapter \ref{ch:s6_results} for details.)

All surviving triggers are saved to the \texttt{sngl\_inspiral} table with their template parameters, \ac{SNR}, and end-time. This table is stored in a xml file; the naming convention used is:
\begin{center}
\texttt{\{IFO\}-INSPIRAL\_FIRST\_\{USER-TAG\}-\{GPS-START\}-\{DURATION\}.xml}
\end{center}
All of the \texttt{FULL\_DATA INSPIRAL\_FIRST} files that are created in our analysis are shown in Figure \ref{fig:segment_plot_full}. There will be an equal number of \texttt{BNSINJ} files, the only difference in the names being the \texttt{USER-TAG}.

\texttt{lalapps\_inspiral} is also used to calculate $\chi^2$ values for triggers. However, since this is not used until the second stage of the pipeline, we withold discussion of this until secton \ref{sec:second_inspiral}.

\subsection{First Coincidence}
\label{sec:first_thinca}

Now that we have lists of single-\ac{IFO} triggers we can perform the coincidence test across detectors. This is done by \texttt{lalapps\_thinca}. Each \texttt{thinca} job reads in multiple \texttt{inspiral} files. The number and duration of \texttt{thinca} jobs is determined by coincidence times. One \texttt{thinca} job will be created for every contiguous coincidence time. This is illustrated in Figure \ref{fig:segment_plot_full}. Only H1 and L1 were analyzed for the first $\sim1700\,$s of the analysis. Thus, one \texttt{thinca} job is created for this period, known as H1L1 time. At GPS time $967230087$ Virgo turns on, and for the next $2176\,$s all three interferometers are analyzed. This results in another \texttt{thinca} job being created for this triple-coincidence time. Afterward, L1 turns off, and so an H1V1 job is created for the next $1755\,$s. This is again followed by a period of triple-coincidence time.

Unlike \texttt{tmpltbank} and \texttt{inspiral} jobs, there is no minimum required duration of contiguous analysis time for \texttt{thinca}; in principle, a \texttt{thinca} job could be as short as one second. A maximum duration of $3600\,$s is imposed to protect against memory errors. If a period of coincidence time is greater than $3600\,$s and less than $7200\,$s, however, the time will be evenly split between two \texttt{thinca} jobs. This can be seen in the last triple-coincident segment in Figure \ref{fig:segment_plot_full}. Note that no overlap is needed between \texttt{thinca} jobs.

In order to carry out the coincidence test \texttt{thinca} constructs an error ellipse around each trigger with size $\epsilon_{\mathrm{thinca}}$ using the metric components computed by \texttt{tmpltbank}. The size of $\epsilon_{\mathrm{thinca}}$ is a tunable parameter and is set in the configuration file. Triggers are considered coincident if their ellipses --- referred to in this case as \emph{ethinca ellipsoids} --- overlap. A single trigger can take part in multiple coincidences if it overlaps with multiple triggers. A triple (or higher) coincidence can only occur if all three triggers in each \ac{IFO} overlap with each other. If one trigger overlaps with the other two, but those two do not overlap with each other, two double coincidences will be created.

Any triggers found to be in coincidence with a trigger with at least one other detector are saved to the \texttt{sngl\_inspiral} table in the output xml file. One xml file is created for each \texttt{thinca} job. Note that this means a single \texttt{thinca} xml file will contain triggers from multiple \acp{IFO}; how these are stored in the \texttt{sngl\_inspiral} table is discussed in section \ref{sec:data_storage}. The naming convention for first coincidence files is:
\begin{center}
\texttt{\{IFOS\}-THINCA\_FIRST\_\{USER-TAG\}-\{GPS-START\}-\{DURATION\}.xml}
\end{center}
\texttt{IFOS} is the coincidence time the \texttt{thinca} file covered; it is not necessarily the coincidence types stored in the file.\footnote{For example, an H1L1V1 triple-time file will have prefix \texttt{H1L1V1}, yet can contain up to four coincidence types: H1L1, H1V1, L1V1, and H1L1V1.} Any single-\ac{IFO} trigger that is not coincident with any other \ac{IFO} is discarded.

\texttt{Thinca} also has the ability to apply higher-category vetoes and do time slides. As this is not done until the second stage, we withold discussion of this until section \ref{sec:second_thinca}.

\subsection{Trigbank}
\label{sec:tribank}

The completion of first coincidence concludes the first stage of the \ac{HIPE} pipeline. To carry out the second stage of the pipeline, we must first gather all surviving triggers in preparation for the second run of \texttt{lalapps\_inspiral}. This is done by \texttt{lalapps\_trigbank}. The number of \texttt{trigbank} jobs is determined by the number of single-\ac{IFO} analysis chunks and the number of coincidence times. Within each analysis chunk, a separate \texttt{trigbank} job is created for each coincidence time that exists in that chunk and for each \ac{IFO}. For example, in our toy analysis, the first H1 analysis chunk overlaps H1L1 time and H1L1V1 time. Therefore, two \texttt{trigbank} jobs are created for H1 during this time, one for each coincidence time.

Each \texttt{trigbank} job loads all first \texttt{thinca} files of a given coincidence time that overlap its analysis chunk. All triggers from a single \ac{IFO} are picked out of the \texttt{thinca} files and redundancies are removed (e.g., if a single template rang off multiple triggers in the \texttt{thinca} file, only one entry representing that template will be saved). The results are then saved to a \texttt{sngl\_inspiral} table in a single \texttt{TRIGBANK} file; the naming convention is:
\begin{center}
\texttt{\{IFO\}-TRIGBANK\_SECOND\_\{IFO-TIME\}\_\{USER-TAG\}-\{GPS-START\}-\{DURATION\}.xml}
\end{center}
\texttt{IFO-TIME} is the coincident time from which the triggers in the file came. By adding this tag we ensure that a different file exists for each coincidence time. Figure \ref{fig:segment_plot_full} shows all of the \texttt{FULL\_DATA TRIGBANK} files created in our toy analysis.

\subsection{Second Inspiral}
\label{sec:second_inspiral}

After the trigbank files have been generated, \texttt{lalappps\_inspiral} is run again. As with first \texttt{inspiral}, the second run loads the strain data from the frame files, conditions the data, adds injections (for injection runs), and match filters the data. There are two important differences: first, rather than load a \texttt{TMPLTBANK} file, a \texttt{TRIGBANK} file is used. Second, $\chi^2$ is now computed along with \ac{SNR}, and a $\chi^2$ threshold and $r^2$ veto are applied to triggers.

$\chi^2$ is computed for any trigger that exceeds the \ac{SNR} threshold. (Note that triggers have been defined before this happens; i.e. the max-over chirp-length algorithm is still based solely on \ac{SNR}.) As per the equations outlined in section \ref{sec:chisq} of Chapter \ref{ch:pipeline_principles}, this is done by breaking the template up into bins of equal power, match filtering each bin individually, and comparing the \ac{SNR} in each bin to the expected \ac{SNR} in a single bin. The number of bins used is set in the configuration file. For \ac{S5} and \ac{S6} 16 bins were used, making the number of degrees of freedom equal to 30. This process is computationally expensive; hence why it is only used for triggers that survive first coincidence. Once the $\chi^2$ value for a trigger is computed, the $\chi^2$ threshold is applied. While the exct value of the $\chi^2$ threshold is \ac{SNR} dependent, the value of $\chi_*^2$ (see equation \ref{eqn:chisq_threshold} in Chapter \ref{ch:pipeline_principles}) is a tuneable parameter that is set in the configuration file. If a trigger has a $\chi^2$ that exceeds the $\chi^2$ threshold, it is discarded. \texttt{Inspiral} also computes an $r^2$ value for each trigger across a period of time that is also set in the configuration file. If the $r^2$ value exceeds a threshold $r_*^2$ determined in the configuration file, the trigger is discarded. For values of $\chi_*^2$, $r_*^2$, and the size of the $r^2$ window used for \ac{S5} and \ac{S6}, see Chapters \ref{ch:s5_results} and \ref{ch:s6_results}. \texttt{Inspiral} does not compute effective \ac{SNR} nor new \ac{SNR}. Instead, the $\chi^2$ value for surviving triggers and the number of degrees of freedom are saved to the \texttt{sngl\_inspiral} table along with the triggers' other information (\ac{SNR}, template parameters, end-time). Later programs use this information to compute effective or new \ac{SNR} as needed.

Clustering across the bank (either by the hard-window method or trigscan) is still based on \ac{SNR}. However, because this occurs after the $\chi^2$ and $r^2$ vetoes are applied, the trigger that survives bank clustering may not be the same as in first \texttt{inspiral}. For example, a glitch or signal may ring off a template with a large \ac{SNR} but a poor $\chi^2$. If this template had the largest \ac{SNR} across the bank, it would have been selected in the first stage. However, if the template's $\chi^2$ or $r^2$ value exceeds the veto threshold, it will not survive to the bank clustering phase, and so another template will be selected. 

As can be seen in Figure \ref{fig:segment_plot_full}, one second \texttt{inspiral} job is run for every \texttt{trigbank} file. Since there are many more \texttt{trigbank} files then \texttt{tmpltbank} files, this may seem to defeat the purpose of limiting the number of triggers for which $\chi^2$ is calculated. However, templates in a given \texttt{trigbank} file are only filtered through segments that overlap with the corresponding \texttt{thinca} file  (the entire chunk is used for to compute the \ac{PSD}). For example, in our toy analysis, there are two \texttt{trigbank} files --- and therefore two second \texttt{inspiral} jobs --- for the first H1 analysis chunk. This was because part of the way through the chunk, V1 turned on, and so we switched from H1L1 time to H1L1V1 time. The second \texttt{inspiral} job that reads in the H1L1 \texttt{trigbank} file in this segment will only match filter the segments that overlap the H1L1 coincidence time (which is the first $1672$ seconds of the chunk). Likewise, the second second \texttt{inspiral} job in this chunk, which is associated with the H1L1V1 \texttt{trigbank} file, will only match filter the segments that overlap H1L1V1 time (which is the last $376$ seconds). For this reason, the naming convention for second \texttt{inspiral}'s output xml files is:
\begin{center}
\texttt{\{IFO\}-INSPIRAL\_SECOND\_\{IFO-TIME\}\_\{USER-TAG\}-\{GPS-START\}-\{DURATION\}.xml}
\end{center}
By limiting the filtering in this manner we minimize the number of triggers for which we need to compute a $\chi^2$ value.

\subsection{Second Coincidence}
\label{sec:second_thinca}

Since many triggers will have failed the $\chi^2$ and $r^2$ vetoes in second \texttt{inspiral} (and since slightly different templates may have taken their place) we must again perform the coincidence test. This is again carried out by \texttt{lalapps\_thinca} with many of the same arguments as before. In fact, the CAT 1, zero-lag coincidence job will perform exactly the same operations as its first stage counterpart; even the start/end times (which correspond to the coincidence times) will be the same. The only difference is the second stage coincidence will read in the corresponding \texttt{INSPIRAL\_SECOND} files.

It is at this point that we perform time-slides and apply higher-category (i.e., $>$ CAT1) vetoes. This results in several additional \texttt{thinca} jobs than were carried out at first stage. Specifically, for a non-injection run, there will one zero-lag and one slide \texttt{thinca} job for each category of veto analyzed.\footnote{Although there are, in principle, up to four categories of vetoes --- see table \ref{tab:veto_cats} --- not all of them have to be analyzed by \ac{HIPE}. The specific categories that we wish to analyze are determined by the configuration file.} We do not perform slides for injection runs, but we do apply higher vetoes.

Higher category vetoes are applied by feeding \texttt{lalapps\_thinca} an ASCII file containing the list of veto segments to apply for a given \ac{IFO} (and by turning on an extra \texttt{--do-veto} argument). These ASCII files are created by \ihope at run-time from the veto xml files created by \texttt{ligolw\_segments\_from\_cats}\footnote{\texttt{Thinca} does not read the xml files directly because it was written several years before \texttt{ligolw\_segments\_from\_cats}. Rather than try to adjust \texttt{thinca} to read the new xml files, we found it easier to simply convert the xml to ASCII format.}, and are stored in the \texttt{segments} directory. A separate ASCII file is given to \texttt{thinca} for each \ac{IFO}. If a veto file for an \ac{IFO} is specified, \texttt{thinca} loads it, then removes all triggers from the given \ac{IFO} that have end times intersecting the veto segments. This is done prior to performing the coincidence test. As mentioned above, vetoes are applied cumulatively. Thus, the CAT2 veto file will contain the union of CAT 1 and 2 veto segments; CAT3, the union of 1, 2, and 3; etc. For this reason, \texttt{thinca} only needs to load one veto file per \ac{IFO} per job.

Slides are carried out by giving \texttt{thinca} a \texttt{--num-slides} argument, followed by arguments giving the relative offset to apply for each \ac{IFO}. \texttt{Thinca} uses these arguments to construct offset vectors for each slide. The offset values for each \ac{IFO} in a given slide are determined by the slide number and the relative offsets of the \acp{IFO}. For example, if the H1 offset is 0 (set via the \texttt{--h1-slide} argument), the L1 offset is 5 (via \texttt{--l1-slide}), and V1 offset is 10 (\texttt{--v1-slide}), then for the third slide, the offset vector will be:
\begin{equation*}
\vec{\mathcal{O}}_3 = [\Delta t_{\mathrm{H1}} ~ \Delta t_{\mathrm{L1}} ~ \Delta t_{\mathrm{V1}}] = [0\,\mathrm{s} ~ 15\,\mathrm{s} ~ 30\,\mathrm{s}] 
\end{equation*}
The \texttt{num-slides} argument sets the total number of slides to perform. The number of slides will be twice the value given by this argument: one set of forward slides, and one set of backward slides. For example, if \texttt{num-slides} is 20, 40 total slides will be created: 20 slides with positive offsets and 20 slides with negative offsets. The number of slides to perform, and the offset for each \ac{IFO} is set in the configuration file. For values set for \ac{S5} and \ac{S6}, see Chapters \ref{ch:s5_results} and \ref{ch:s6_results}, respectively.

Both the \texttt{num-slides} and offset arguments must be integers. This means that \texttt{thinca} cannot perform slides with offsets that are fractions of a second. Furthermore, due to the way \texttt{thinca} constructs the offset vectors, it is not possible to mix positive offsets with negative, nor is it possible to have two separate vectors in which two of the \acp{IFO} have the same offset (unless the two \acp{IFO} have the same relative offset). For example, \texttt{thinca} cannot create the vectors $[\mathrm{H1} = 0\,\mathrm{s} ~ \mathrm{L1} = 5\,\mathrm{s} ~ \mathrm{V1} = 10\,\mathrm{s}]$ and $[\mathrm{H1} = 0\,\mathrm{s} ~ \mathrm{L1} = 5\,\mathrm{s} ~ \mathrm{V1} = 20\,\mathrm{s}]$. This limits the total number of slides \texttt{thinca} can perform for times in which there are more than two coincident \acp{IFO}.

For each slide, \texttt{thinca} adds the offsets to the end times of \acp{IFO} then performs the coincidence test. This is done after vetoes are applied, so that higher-category vetoes are effectively slid around with the offset, too. Slides are performed on a \emph{ring}: triggers that are slid past the end time of the \texttt{thiinca} job are placed at the beginning of the time. This means that a \texttt{thinca} slide job does not have to load triggers from any other segment. It also means that triggers occuring in single-\ac{IFO} time cannot be slid into coincidence time. Thus we are safe discarding triggers that occur during single-\ac{IFO} time after the first stage coincidence test. Since there is no minimum required duration for a \texttt{thinca} file, this can mean that some slides will be redundant if a file is too short. Studies have shown that this is rare, however, and so it has little effect on the background analysis.

The naming conventions for second \texttt{thinca} output xml files are:
\begin{center}
\footnotesize{\texttt{\{IFO-TIME\}-THINCA\_SECOND\_\{IFO-TIME\}\_\{USER-TAG\}-\{GPS-START\}-\{DURATION\}.xml}}
\end{center}
for the CAT1 zero-lag files, and:
\begin{center}
\footnotesize{\texttt{\{IFO-TIME\}-THINCA\_SECOND\_\{IFO-TIME\}\_\{USER-TAG\}\_CAT\_\{N\}\_VETO-\{GPS-START\}-\{DURATION\}.xml}}
\end{center}
for the N$\th$-higher veto category.\footnote{Note that the second \texttt{IFO-TIME} in the naming convention is unnecessary; it is simply a relic of the \texttt{trigbank} and second \texttt{inspiral} naming conventions and has no other meaning.} Slide files follow the same convention, except that \texttt{THINCA\_SECOND} is replaced with \texttt{THINCA\_SLIDE\_SECOND}.

One important final note: by definition, when an \ac{IFO} is vetoed, it is no longer considered to be ``on." This means that when we apply the higher-category vetoes, coincidence times will change. For instance, if a category-2 L1 veto comes on for $t$ seconds during H1L1V1 time, then at CAT2 (and all higher cumulative categories) that period of time is now H1V1 time. The same rule applies to time slides. Second \texttt{thinca} files, however, are grouped by whatever the coincidence time was at CAT1, and this does not change after vetoes are applied (we seen this in Figure \ref{fig:segment_plot_full}: all the \texttt{THINCA\_SECOND} files have the same start and end times across all veto categories). This means that for higher veto categories, a \texttt{THINCA\_SECOND} file will contain multiple coincidence times. Unfortunately, \texttt{thinca} does not store which triggers fall in which coincidence times, nor does it store the duration of each of the coincidence times in its output file. As discussed in Chapter \ref{ch:far}, knowing the start and end of coincidence times is important information: uncombined \acp{FAR} are computed for each coincidence type, and \acp{FAR} are not combined across coincidence time. Thus later programs must re-apply the vetoes to sort out the triggers and times. This is done by \texttt{ligolw\_thinca\_to\_coinc}, which is the first program to run in pipedown (see section \ref{sec:pipedown}).

\begin{figure}[p]
\label{fig:science-selected_segs}
\begin{center}
\includegraphics[width=6in]{figures/segment_plot_science-selected.pdf}
\end{center}
\caption{
The Science, category 1 vetoes, and selected segments of H1, L1, and V1 between GPS times 967228343 and 967238583.}
\end{figure}

\begin{figure}[p]
\label{fig:segment_plot_full}
\includegraphics[width=6.5in]{figures/segment_plot_selected_segs-thinca_second.pdf}
\caption{
The selected segments and all zero-lag \texttt{FULL\_DATA} files created by \ac{HIPE} for the segments shown in Figure \ref{fig:science-selected_segs}.}
\end{figure}

\section{Data Storage}
\label{sec:data_storage}

Before stepping through Pipedown, we pause to detail the table structure used to store triggers in files. Table structure is central to Pipedown: part of its purpose is to convert from older table structures to newer, more convenient ones, and all the programs Pipedown runs take advantage of these newer tables. We begin by reviewing the \texttt{sngl\_inspiral} table, and how this was used to store data up to this point.

\subsection{The \texttt{sngl\_inspiral} Table}

As discussed above, the \texttt{sngl\_inspiral} table is the table used to store data by nearly every \ac{HIPE} program: \texttt{lalapps\_tmpltbank}, \texttt{lalapps\_inspiral}, and \texttt{lalapps\_thinca} all use it to store template and trigger information. Consequently, the \texttt{sngl\_inspiral} table has a large number of columns to store all needed data. Table \ref{tab:sngl_inspiral} shows some of the relevant columns used for the \ac{CBC} analysis and their purpose. There are several extra columns not discussed here.

Each row in the \texttt{sngl\_inspiral} table represents a single trigger or template in a single \ac{IFO}. Associated with each row is an \texttt{event\_id} to uniquely identify each event. This provides a convenient way to retrieve information about a template or single-\ac{IFO} trigger. Coincident information is more difficult to store, however, since coincidence requires grouping rows together. Since the \texttt{sngl\_inspiral} table was the only available way to store trigger data in a file when \texttt{lalapps\_thinca} was written, a method was devised to encode coincidence in the \texttt{event\_id}. The id was set to be 18 digits long. The first (starting from the left) 9 digits were the GPS time of the trigger. The next 4 digits gave the slide that the coincidence was from, where $0000$ was zero-lag. The last 5 digits formed a counter that assigned a unique number to each coincidence. For example, the first coincident trigger in H1L1 time in the first slide of our toy-analysis has \texttt{event\_id}:
\begin{equation*}
\underbrace{956735064}_{\text{GPS time}}\underbrace{0001}_{\text{slide number~}}\underbrace{00001}_{\text{~event counter}}
\end{equation*}
Any rows that had \texttt{event\_id}s with the same last 9 digits --- i.e., if the slide number and event counter matched --- were coincidences.

\subsection{Coinc Tables}

There were a few problems with using the \texttt{sngl\_inspiral} table to store coincident information. First, the number of coincident events in a single file could not exceed $99999$, else the event counter would spill over into the slide number. Likewise, no more than $9999$ slides could be stored in a single file. Third, it consumed more memory than necessary. If a single-\ac{IFO} trigger formed coincidences in multiple slides (as is often the case), multiple entries would have to be created in the table for it. The values in all columns (of which there are many) for these entries would be exactly the same, except for the last 9 digits of the \texttt{event\_id}. Finally, any coincident parameters, such as combined new \ac{SNR}, would have to be computed on the fly. For coincident statistics that could not be computed on the fly, such as \acp{FAR}, a \texttt{sngl\_inspiral} column (meant to store single information) had to be comandeered, and the same statistic would be stored repeatedly for each \ac{IFO} in the coincidence.

To remedy the situation, \emph{coinc tables} were developed. The coinc tables consist of the \texttt{coinc\_inspiral}, \texttt{coinc\_event}, \texttt{coinc\_event\_map}, and the \texttt{coinc\_definer} table. Additionally, there is the \texttt{time\_slide} table, although this is considered to be apart of the \emph{experiment} tables (see the next section). The columns in each of these tables and their purpose is shown in tables \ref{tab:coinc_inspiral} -- \ref{tab:time_slide}.

The \texttt{coinc\_inspiral} table is the coincident analog to the \texttt{sngl\_inspiral} table. Instead of storing single-\ac{IFO} information, the \texttt{coinc\_inspiral} table stores combined statistics and parameters. How information is combined depends on the parameter or statistic. Combined new \ac{SNR} is calculated by taking the quadruture sum of the constituent \ac{IFO}'s new \acp{SNR}, and is stored in the \texttt{snr} column.\footnote{Admittedly, storing combined new \ac{SNR}, not combined \ac{SNR}, in a column named \texttt{snr} is a little misleading. When the column names were defined, the \texttt{snr} column was intended to be a catch-all for whatever statistic was used to rank triggers. This way, multiple searches that may not use the same ranking statistic could use the same column. In retrospect, however, doing it in this manner was probably not needed, since SQLite allows columns to be added at will (see section \ref{sec:file_formats}). At the very least, the column probably should have been called something along the lines of \texttt{ranking\_stat}. Alas, hindsight is always 20/20.} Intrinsic parameters of the templates, such as chirp mass (stored in the \texttt{mchirp} column) and total mass (stored in the \texttt{mass} column), are combined by taking the mean of the of single-\ac{IFO} parameters. Coincident end times are computed by taking the end time from the first \ac{IFO} when sorted alphabetically. For example, in an H1L1 coincidence, the coincident end time will be whatever the H1 end time is. In an L1V1 coincidence, the L1 end time will be used. Similar to the original intent of the \texttt{event\_id}, a unique \texttt{coinc\_event\_id} is assigned to each coincidence in a file to provide a quick way to identify them. Additionaly, the \texttt{coinc\_event\_id} is used to map entries in the \texttt{coinc\_inspiral} table to entries in other tables. For every entry in the \texttt{coinc\_inspiral} table there is one entry in the \texttt{coinc\_event} table, as well as multiple entries in the \texttt{coinc\_event\_map} table with the same \texttt{coinc\_event\_id}.

The \texttt{coinc\_event\_map} table is used to map the coincident events in the \texttt{coinc\_inspiral} table to their constituent events in the \texttt{sngl\_inspiral} table. As can seen in table \ref{tab:coinc_event_map}, the \texttt{coinc\_event\_map} table has three columns: a \texttt{coinc\_event\_id}, an \texttt{event\_id}, and a \texttt{table\_name}. To map a single-\ac{IFO} trigger in teh \texttt{sngl\_inspiral} table to a coincident event, the single event's \texttt{event\_id} is entered in the \texttt{coinc\_event\_map} table next to the \texttt{coinc\_event\_id} of the coincidence it is part of. Likewise, all other single-ac{IFO} events that are apart of the coincidence will have entries in the \texttt{coinc\_event\_map} table with the same \texttt{coinc\_event\_id}. In this way, one coincident event is mapped to multiple single events. Likewise, a single \texttt{event\_id} does not have to be associated with one \texttt{coinc\_event\_id}. If a single event takes part in multiple coincidences it can also be added to the \texttt{coinc\_event\_map} table, with each entry being associated with a different \texttt{coinc\_event\_id}. By using an intermediate table we have allowed for many-to-many mappings between coincidences and single events. As a result, multiple entries do not have to be added to the \texttt{sngl\_inspiral} table, and the \texttt{event\_id}s no longer need to encode coincident information. \texttt{Event\_ids} are therefore changed to simple counters used to index the \texttt{sngl\_inspiral} table, so that each row has a unique id. This fixes the problem of counter-overflow; the number of events that can be stored in a single file is now only limited by disk or memory space.

Note that the table name from which a single event came --- in this case, the \texttt{sngl\_inspiral} table --- is also stored in the \texttt{coinc\_event\_map} table. This is done so that the \texttt{coinc\_event\_map} table can be used to draw multiple types of maps, e.g., it is also used to draw mappings between coincident events and injections. The manner in which injection mappings are drawn is more complicated, however; see section \ref{sec:inspinjfind} for details.

By changing the \texttt{event\_id} to a simple counter, we have also freed it from storing time-slide information. What slide an event belongs to is instead stored in the \texttt{time\_slide} table. As listed in table \ref{tab:time_slide}, the \texttt{time\_slide} table has an \texttt{ifo}, an \texttt{offset}, and a \texttt{time\_slide\_id} column. Each slide is given a unique \texttt{time\_slide\_id}; each slide id is repeated in the \texttt{time\_slide} table for every \ac{IFO} that was analyzed, with each row giving the offset (in seconds) applied to that \ac{IFO} in that slide. For example, if H1, L1, and V1 are being analyzed, and one of the slides has the offset vector given in equation \ref{eqn:example_offset_vec}, then that slide will have three entries in the time slide table:
\begin{center}
\begin{tabular}{ c | c | c }
time\_slide\_id & ifo & offset \\
\hline\hline
\texttt{time\_slide:time\_slide\_id:37} &   \texttt{H1}    & \texttt{0} \\
\texttt{time\_slide:time\_slide\_id:37} &   \texttt{L1}    & \texttt{15} \\
\texttt{time\_slide:time\_slide\_id:37} &   \texttt{V1}    & \texttt{30} \\
\end{tabular}
\end{center}
where we have assigned the slide the arbitrary id \texttt{time\_slide:time\_slide\_id:37}.\footnote{All ids have the basic format \texttt{[originating\_table]:[id\_name]:[index\_number]}. The index number is what makes each id unique within a given class.} Storing time-slides in this manner makes it easier to find what offset was applied to each \ac{IFO}, and it allows for non-multiple slide vectors should a new version of \texttt{thinca} be written. Coincident events (and, via the \texttt{coinc\_event\_map} table, single events) are mapped to time slides through two parallel tracts. One path is through the \texttt{coinc\_event} table; the other is by way the \texttt{experiment\_map} and \texttt{experiment\_summary} tables, which are discussed in the next section.

The \texttt{coinc\_event} table serves to store additional information about coincident events, as well as map events to other tables. Table \ref{tab:coinc_event} shows the \texttt{coinc\_event}'s columns. Every coincident event will have one entry in the \texttt{coinc\_event} table via their \texttt{coinc\_event\_id}. Since the table also has a \texttt{time\_slide\_id} column, it maps the events to their respective time-slides (a many-to-one mapping). The table additionally has an \texttt{instruments} column which gives the instrument time that the coincident event occurred in, which is important for computing \acp{FAR}. Instrument times are also stored in the \texttt{experiment} table, which is discussed in the next section. The \texttt{likelihood} column is used if a likelihood statistic is computed in a search. We do not do this in the low-mass \ac{CBC} search, however, and so it is unused. That it exists in the table is due to the fact that the \texttt{coinc\_event} table, along with the \texttt{coinc\_event\_map}, are search-independent tables. That is, they can be (and are) used by multiple search pipelines both within the \ac{CBC} group (such as the ringdown search) and outside of it; e.g., some burst searches use it. Contrast this with the \texttt{coinc\_inspiral} and \texttt{sngl\_inspiral} tables, which are specific to \ac{CBC} searches. An advantage of having search independent tables is that provides an easy way to store data if search is carried out that draws from multiple pipelines. Indeed, being able to combine results from multiple pipelines is an active field of research.

The \texttt{coinc\_event} table also maps coincident events to the \texttt{coinc\_definer} table via the \texttt{coinc\_def\_id} column. The \texttt{coinc\_definer} table is used to group together various types of mappings; its columns are shown in table \ref{tab:coinc_definer}. A human-redable description of the type of mappings is given in the \texttt{description} column. For example, all coincident events that are mapped to single-inspiral triggers will be associated with a single \texttt{coinc\_def\_id}. The entry in the \texttt{description} column for this id will be \texttt{sngl\_inspiral<-->sngl\_inspiral coincidences}. Mappings between injections (which are stored in the \texttt{sim\_inspiral} table) and coincident events will have a different \texttt{coinc\_def\_id}, and the \texttt{description} column will be \texttt{sim\_inspiral<-->coinc\_event coincidences}. The \texttt{coinc\_definer} table is therefore useful to grab coincidences formed by a specific method. For example, if two types of injection finding were being experimented with (say, a time-based method and an ethinca-ellipsoid based method), the \texttt{coinc\_definer} table would be needed to pick out which mappings occurred from which method. However, since the current low-mass \ac{CBC} search only performs one type of coincidence test and one type of injection finding (see section \ref{sec:inspinjfind} for details), the \texttt{coinc\_definer} table is largely unused by Pipedown programs.

\subsection{Experiment Tables}
\label{sec:experiment_tables}

The coinc tables solved many of the problems and difficulties of saving coincident information to a file. However, neither the older table structure nor the coinc tables provided a way to store information about the experiments performed. In particular, there was no way to store the live-time of a slide, nor the \emph{data type} of a trigger, i.e., whether the trigger came from playground data, data with playground excluded, time-slides, or an injection run. Initially, there was also no way to store what instrument time a trigger was in. This was amended by adding the \texttt{instruments} column to the \texttt{coinc\_event} table. However, this meant that information about an instrument time would only be stored in the file if an event occurred during it. If we perform an experiment and nothing happens during it, that is still information we want to keep. This is particularily true for computing false alarm rates: we need to know all the time that was observed in every slide, regardless of whether or not a slide produced an event. The only way to keep track of triggers' data types was to keep them in different files, and the only way to store livetimes was to store them in auxillary ASCII files. This could be confusing, and it forced meta-data to stored in file names.

For these reasons, the \emph{experiment} tables were developed. The experiment tables consist of the \texttt{experiment}, \texttt{experiment\_summary}, and \texttt{experiment\_map} tables. Their columns are listed in tables \ref{tab:experiment} -- \ref{tab:experiment_map}. The \texttt{experiment} table lists the experiments that were performed as well as information about each one. The \texttt{gps\_start\_time} and \texttt{gps\_end\_time} columns give the period across which the experiment was performed. In \ihope, these columns are set whatever the start and stop times were that were given to \ihope to analyze. The \texttt{search\_group} and \texttt{search} columns define what group performed the search (e.g., the \ac{CBC} group, or the Burst group) and what type of search was performed (e.g., \texttt{lowmass}). Since different instrument times are mutually exclusive, and since we do not calculate \acp{FAR} across them, every instrument time that was analyzed in the \texttt{search} by the \texttt{search\_group} between the GPS start and end times is listed as an independent experiment in the \texttt{experiment} table. Evidently, \texttt{experiment} not only lists the experiments performed, it \emph{defines} what an experiment is.

Within each experiment, we apply various vetoes, perform slides, and construct different data types (e.g., playground and injection runs). Each of these actions results in a different realization of an experiment. To define and list these realizations, we use the \texttt{experiment\_summary} table. Table \ref{tab:experiment_summary} shows the columns of the \texttt{experiment\_summary} table. Each row has its own \texttt{time\_slide\_id} and \texttt{experiment\_id} which maps it to the \texttt{experiment} and \texttt{time\_slide} table; thus, each row gives information about every slide that was performed in each instrument time. Additionally, there is a \texttt{datatype} column to enumerate each of the data types constructed in every experiment. This can have one of five entries: \texttt{all\_data}, \texttt{playground}, \texttt{exclude\_play}, \texttt{slide}, and \texttt{simulation}. The \texttt{all\_data}, \texttt{playground}, \texttt{exclude\_play}, and \texttt{simulation} are all zero-lag data types: \texttt{playground} is data consisting only of times occuring during playground seconds (defined above), \texttt{exclude\_play} is the converse, and \texttt{all\_data} consists of both. The \texttt{slide} data type is any non-zero lag data. We do not define a \texttt{slide} playground because playground slides are rarely used.\footnote{Looking at all-data slides is not considered opening the box.}

The \texttt{simulation} data type describes zero-lag data that has software injections in it.\footnote{Note that we do not have a simulation slide data type. While it would be more realistic to do slides with injections in them, it would only be useful if the slides had one or two injections in them. This means we would have to re-do the slides for every software injection we create, which would result in an unwieldly amount of data.} This allows us to easily separate injection runs from non-injection runs. In order to separate various injection runs we additionally have the \texttt{sim\_proc\_id} column.

\begin{table}[p]
\label{tab:sngl_inspiral}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{ifo}            &   Name of \ac{IFO} the trigger/template belongs to, e.g., \texttt{H1}. \\
\hline
\texttt{end\_time}      &   End time (in integer GPS seconds) of the trigger (not used by \texttt{tmpltbank}). \\
\hline
\texttt{end\_time\_ns}  & The fractional seconds of the end time, in nanoseconds. \\
\hline
\texttt{template\_duration} & The duration of the template (in seconds). \\
\hline
\texttt{eff\_distance}      & The effective distance to the binary (in $\Mpc$). For templates, this is set to 1; for triggers, this is calculated using equation \ref{eqn:DtoRho}. \\
\hline
\texttt{coa\_phase}     & The phase of the binary at coalescence. \\
\hline
\texttt{mass1}      & The component mass of one of the objects in the binary (in $\Msun$). \\
\hline
\texttt{mass2}      & The other component mass. \\
\hline
\texttt{mtotal}     & The total mass of the binary (in $\Msun$). \\
\hline
\texttt{mchirp}     & The chirp mass, $\mathcal{M}$, of the binary (in $\Msun$). \\
\hline
\texttt{eta}        & The symmetric mass ratio, $\eta$, of the binary. \\
\hline
\texttt{tau0}       & $\tau_0$ (see equation \ref{eqn:tau0tau3}) \\
\hline
\texttt{tau3}       & $\tau_3$ (see equation \ref{eqn:tau0tau3}) \\
\hline
\texttt{snr}        & The \ac{SNR}, $\rho$, of the trigger. \\
\hline
\texttt{chisq}      & The $\chi^2$ value of the trigger. \\
\hline
\texttt{chisq\_dof} & The number of $\chi^2$ degrees of freedom. \\
\hline
\texttt{Gamma[0-9]} & The components of the bank metric at the trigger/template. (There are 9 of these). \\
\hline
\texttt{process\_id}        &   Unique value used to map the trigger/template to the process that created it. \\
\hline
\texttt{event\_id}  & A unique value to identify the event. In \ac{HIPE} this is used to draw coincidences. In Pipedown it is used as an index of the table.
\end{tabular}
\caption{Commonly used columns of the \texttt{sngl\_inspiral} table. Not all columns are shown.}
\end{table}

\begin{table}[h]
\label{tab:coinc_inspiral}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{ifos}   &   The \acp{IFO} that took part in the coincidence. This is comma-separated, e.g. \texttt{H1,L1}. \\
\hline
\texttt{end\_time}  &   The coincident end time (in integer GPS seconds). \\
\hline
\texttt{end\_time\_ns} &    The fractional seconds of the coincident end time (in nanoseconds). \\
\hline
\texttt{mchirp} &   The combined chirp mass, $\mathcal{M}_c$, of the coincidence. \\
\hline
\texttt{minimum\_duration}  &   The duration of the shortest template in the coincidence (in seconds). \\
\hline
\texttt{snr}    &   The combined \emph{new \ac{SNR}}, $\rhonewc$, (not \ac{SNR}) of the coincidence. \\
\hline
\texttt{false\_alaram\_rate} & The uncombined \ac{FAR} of the coincidence (in $\yr^{-1}$). \\
\hline
\texttt{combined\_far}  & The combined \ac{FAR} of the coincidence (in $\yr^{-1}$). \\
\hline
\texttt{coinc\_event\_id}   & A unique value to identify the coincidence.
\end{tabular}
\caption{The columns of the \texttt{coinc\_inspiral} table and their purpose.}
\end{table}

\begin{table}[h]
\label{tab:coinc_event_map}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{coinc\_event\_id}   &   A unique value to identify coincidences and to map them to single-\ac{IFO} events. \\
\hline
\texttt{event\_id}  & The id of one of the coincident event's consituents. This may not necessarily point to a column named ``event\_id"; e.g., to map an injection to a coincident events, the injection's \texttt{simulation\_id} will be stored here. \\
\hline
\texttt{table\_name}    &   The name of the table that the \texttt{event\_id} is found in.
\end{tabular}
\caption{The columns of the \texttt{coinc\_event\_map} table and their purpose.}
\end{table}

\begin{table}
\label{tab:coinc_event}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{coinc\_event\_id}   &   Used to map entries to coincident events. \\
\hline
\texttt{time\_slide\_id}    &   Used to map entries to a slide (see \texttt{time\_slide} table). \\
\hline
\texttt{coinc\_def\_id}     &   Used to map entries to a coinc-definer entry (see \texttt{coinc\_definer} table). \\
\hline
\texttt{instruments}        &   Instruments that were on at the time of the coincidence. Also stored in the \texttt{experiment} table. \\
\hline
\texttt{likelihood}         &   Likelihood statistic of the coincidence. This is not used for low-mass \ac{CBC} searches. \\
\hline
\texttt{process\_id}        &   Unique value used to map coincidences to the process created them.
\end{tabular}
\caption{The columns of the \texttt{coinc\_event} table and their purpose.}
\end{table}

\begin{table}
\label{tab:coinc_definer}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{search}     &       Name of search that produced a class of coincidences, e.g., \texttt{inspiral}. \\
\hline
\texttt{description}    &   Human-readable entry describing the type of coincidence, e.g., \texttt{sim\_inspiral<-->coinc\_event coincidences}. \\
\hline
\texttt{search\_coinc\_type}    &   Integer to identify the type of coincidence. \\
\hline
\texttt{coinc\_def\_id}     &   Unique value used to index each row in the table.
\end{tabular}
\caption{The columns of the \texttt{coinc\_definer} table and their purpose.}
\end{table}

\begin{table}
\label{tab:time_slide}
\center
\begin{tabular}{m{3.5cm} | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{instrument} &   One of the analyzed \acp{IFO}. \\
\hline
\texttt{offset}     &   The offset (in seconds) applied to the \ac{IFO}. \\
\hline
\texttt{time\_slide\_id}    &   Used to identify a slide; each slide has a unique id. \\
\hline
\texttt{process\_id}    &   Unique value used to map the slides to the process that created them.
\end{tabular}
\caption{The columns of the \texttt{time\_slide} table and their purpose.}
\end{table}

\begin{table}
\label{tab:experiment}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{experiment\_id}  &  Unique id to index the row. \\
\hline
\texttt{instruments}    &  The instruments that were on for the experiment, e.g., \texttt{H1,L1,V1}. If a coincident event happens during the experiment, then this is the instrument-time of the event.\\
\hline
\texttt{search\_group}  &  The search group that carried out the experiment, e.g., \texttt{cbc}. \\
\hline
\texttt{search}     &   The type of search done, e.g., \texttt{lowmass}. \\
\hline
\texttt{gps\_start\_time}   &   The GPS start time of the experiment. \\
\hline
\texttt{gps\_end\_time}     &   The GPS end time of the experiment. \\
\hline
\texttt{comments}   &  Add a comment to the experiment. (Not typically used.) \\
\hline
\texttt{lars\_id}   & The LARS id of the experiment. This is a number meant to index all searches. \\
\end{tabular}
\caption{The columns of the \texttt{experiment} table and their purpose.}
\end{table}

\begin{table}
\label{tab:experiment_summary}
\center
\begin{tabular}{l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{experiment\_summ\_id}   &   Unique id to index the row. \\
\hline
\texttt{experiment\_id}     &   Maps the row to an experiment in the \texttt{experiment} table. \\
\hline
\texttt{time\_slide\_id}    &   Maps the row to a slide in the \texttt{time\_slide} table. \\
\hline
\texttt{veto\_def\_name}    &   The name of the vetoes applied, e.g., \texttt{VETO\_CAT3\_CUMULATIVE}. This can be used to map the row to the collection of veto segments to the \texttt{segment\_definer} table. \\
\hline
\texttt{datatype}   &   The data type of the row. This can either be \texttt{all\_data}, \texttt{playground}, \texttt{exclude\_play}, \texttt{slide}, or \texttt{simulation}. \\
\hline
\texttt{sim\_proc\_id}  &   If the data type is \texttt{simulation}, this maps the row to an injection set in the \texttt{sim\_inspiral} table via the set's \texttt{process\_id}. \\
\hline
\texttt{duration}   &   The livetime of the slide. \\
\hline
\texttt{nevents}    &   The number of coincident events that occurred in the slide.
\end{tabular}
\caption{The columns of the \texttt{experiment\_summary} table and their purpose.}
\end{table}

\begin{table}
\label{tab:experiment_map}
\center
\begin{tabular}{ l | p{9cm}}
Column      &   Purpose     \\
\hline \hline
\texttt{experiment\_summ\_id}   &   Maps the row to a particular instantiation of an experiment in the \texttt{experiment\_summary} table. \\
\hline
\texttt{coinc\_event\_id}   &   Maps the row to a coincident event.
\end{tabular}
\caption{The columns of the \texttt{experiment\_summary} table and their purpose.}
\end{table}
