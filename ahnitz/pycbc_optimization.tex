
\section{Introduction}
\vspace*{-5pt}

Compact binary coalescence is the most promising source of gravitational-waves
for Advanced LIGO~\cite{Abadie:2010cf}.  The inspiral and merger of a binary
containing stellar-mass compact objects (neutron stars and black holes)
generates gravitational waves that sweep upward in frequency and amplitude
through the sensitive band of Advanced LIGO. Compact binary coalescence
searches in Initial
LIGO~\cite{LIGOS1iul,Abbott:2005pe,Abbott:2005kq,Abbott:2005qm,Abbott:2007ai,Abbott:2009tt,Abbott:2009qj,Abadie:2010yb,Collaboration:S5HighMass,Colaboration:2011np,Aasi:2013jjl}
were performed by a high-latency (or deep, offline) search pipeline. For Advanced LIGO, both high-latency offline deep searches and low-latency rapid-result searches will be performed. The
offline search pipeline performs: (i) initial analysis of LIGO data with and
without simulated signals to measure detector performance and tune the
parameters for a deep search; (ii) full analysis of the data set with the
final search parameters to detect signals and measure the false alarm rate of
detection candidates; and (iii) reanalyzing the data with the addition of a
large number of simulated signals to determine LIGO's sensitivity and
selection bias to the astrophysical population.
The high-latency CBC search incorporates information not available in the
low-latency search (for example, improved detector calibration information and
data quality information from offline detector characterization). It also
calculates detection probabilities using large,
time-symmetric samples of the detector noise background and performs a deep,
comprehensive search over the full parameter space of compact binaries.
%whereas the low-latency search pipeline only targets a the sources where an
%electromagnetic counterpart is expected.  
%The high-latency pipeline also
%ensures that any data lost to the low-latency pipeline through data dropouts
%or processing errors is included in the final search.  
The offline search 
measures the selection bias of the CBC search, including the loss in detection
efficiency caused by uncertainties in the model waveforms, or the omission of
certain physics (e.g. binary precession) in the detection templates.  In the
absence of a detection in a given region of the parameter space, the deep offline
search pipeline computes upper limits on the rate of CBC sources which can be
used to constrain astrophysical models of binary and compact object formation.
Consequently, both the deep, offline and the low-latency pipelines are
required to achieve Advanced LIGO's science goals. 

This document describes the computational cost of the high-latency CBC search,
which has been substantially re-written for Advanced LIGO. The prioritized request for computing 
for binary neutron star, neutron star--black hole,
and binary black hole sources is summarized in
Table~\ref{t:offline-xsede-request} (in Intel\textsuperscript{\textregistered} E5-2670 core
hours per year). For reference, we include the request from Table~3 of
LIGO-T1400269 presented in May 2014\footnote{The May 2014 offline CBC request
in LIGO-T1400269
was presented in Stampede SU, which assumes that a core is an Intel\textsuperscript{\textregistered} E5-2680.  For
direct comparison in Table~\ref{t:offline-xsede-request}, we convert this
request to E5-2670 cores by multiplying Stampede SU by the ratio of the clock
speeds, i.e. $2.7/2.6$.}.



\begin{table}[!t]
\centering
{\small
\begin{tabular}{|l|l|l|l|}
\hline 
{\bf Astrophysical search target}  & \multicolumn{3}{l|}{\bf E5-2670 MSU per year}\\

{ } &  {\bf 2015--16} & {\bf 2016--17} & {\bf 2017--18}  \\\hline\hline
{ Highest priority:\, Binary neutron stars (non-spinning templates)} & 0.084 & 0.514 & 1.48 \\\hline
{ High priority: Binary neutron stars (aligned-spin templates)} & 1.25 & 8.82 & 30.2 \\\hline
{ \it May 2014 Request: Binary neutron stars (aligned-spin templates)} & \it 44.4 & \it 130 & \it 270 \\\hline\hline
{ Highest priority: Neutron star--black hole (aligned spin templates) } & 2.26 & 17.5 & 65.7 \\\hline
{ \it May 2014 Request: Neutron star--black hole (aligned spin templates) } & \it 47.1 & \it 167 & \it 494 \\\hline\hline
{ Highest priority: Binary black hole search  (aligned spin templates) } & 1.45 & 11.9 & 36.1 \\\hline
{ \it May 2014 Request: Binary black hole search  (aligned spin templates) } & \it 23.5 & \it 72.3 & \it 151 \\\hline\hline
{ \bf Total for all high-latency CBC searches } & \bf 5.04 & \bf 38.7 & \bf 133 \\\hline
{ \it May 2014 Total for all high-latency CBC searches } & \it 115  & \it 369 & \it 915 \\\hline
\end{tabular}
}
\caption{\label{t:offline-xsede-request}
The computational resources needed to achieve the LVC's
production high-latency CBC search in millions of service units (MSU) per
year. One service unit is defined as one core hour on an Intel\textsuperscript{\textregistered} 
E5-2670. 
%Note the non-spinning binary neutron star search is a sub-set of the
%aligned spin binary neutron star search, so only one of these searches will be
%performed. 
Shown below each science goal (in italics) is the size of the
corresponding May 2014 request from Table 3 of LIGO-T1400269; the difference from May 2014 to April 2015 reflect optimization of code and the flow down of science priorities. Since no search
pipeline for precessing searches exists, this search is currently not listed
in the LSC's prioritized science goals.  Large-scale simulations included in
the above cost will allow us to measure the sensitivity of aligned-spin
searches to precessing systems.
}
\end{table}

We note that the computational resources requested here for the
high-latency CBC request are significantly less than those presented in the
May 2014 request, with the 2015/16 request being a factor of 23 smaller and
the 2017/18 request being a factor of 6.8 smaller. The majority of this
reduction is due to the significant improvements we have made in the new PyCBC
search executable, compared to the old LALApps search code. \textbf{The new
PyCBC code is a factor of $\mathbf{6.75}$ faster in terms of search throughput on our
reference CPU platform than search code used in Initial LIGO.} We describe the
optimizations that we have made to achieve this in detail in
Section~\ref{sec:CompAlgorithms}.

Further reductions are due to changes in the size of the template bank used in
the search, which changes computational cost linearly.  As a result of the scientific prioritization process and input
from the astrophysics community, we increased the minimum neutron star mass in
the binary neutron star and neutron star--black hole searches from $0.9\,
M_\odot$ to $1.0\, M_\odot$. This results in a template bank that is a factor of
$\sim 1.3$ smaller than that used in May 2014. Further reductions in the size
of the template bank are due to the use of a more refined noise curve than
that used in May 2014 to model the detector sensitivity.  
%Updated template
%banks computed using the more accurate noise curve models result in a factor
%of between $2.0$ and $1.1$ decrease in computational cost.  
Template banks computed using the more accurate noise model are a
factor of 1.1--2 times smaller.
We have also used a more realistic estimate of the detector observation time,
which reduces the requested computational resources by a factor of 2.4 in
2015--16, 1.8 in 2016--17, and 1.4 in 2017--18. We have also determined that a
larger number of simulated signals will be required to measure the efficiency
of the offline search than accounted for in the May 2014 request. This
increases our request by a factor of 1.6 in 2015--16, 2 in 2016--17, and 2.5
in 2017--18. However, the overall cost is still
substantially smaller than that presented last year.

There are several uncertainties in the computational cost
estimates presented here. The most significant uncertainty is the detector
sensitivity and bandwidth, as described in Section~\ref{sec:justification}. If
detector commissioning progresses at a more rapid pace and we achieve the best
expected Advanced LIGO sensitivity in O3, our computational cost would
increase by $\sim 50\%$. The throughput of the search code also depends on the
(as yet unknown) stationarity of the detector data, with more stationary data
having a faster throughput. If the data are very clean, then the
computational cost could be $\sim 20\%$ less than requested here. If the data
are very non-stationary, containing many non-Gaussian noise transients, then
the computational cost could increase by $\sim 40\%$. %(However, a reduction of
%this size is unlikely given on the quality of data seen to date from the
%Advanced LIGO detectors). 
We will continue to
refine our computational needs based on instrument progress and our best known
predictions for the detector's sensitivity evolution.

In addition to our optimization work on CPUs, we have also explored the use of
GPUs, which show significant promise for use in the high-latency CBC search.
The fastest throughput we have observed on a GPU with our initial CUDA kernels
is a factor of $\sim 3$ higher than the fastest throughput on a CPU socket. We
have also explored the use of consumer grade GPUs and have demonstrated that
these can yield an order of magnitude greater throughput per dollar than CPUs. Our
initial GPU kernels have not yet been fully optimized. We 
are collaborating with NVIDIA to increase their performance, as
described in Section.~\ref{sec:gpu-trade}.

Finally, we note that the scientific methods used for computational cost estimates
here are the same as those used for searches for gravitational waves
in Initial LIGO.  Reduction in computational costs described below therefore
result from optimization of the existing methods, prioritizing our science,
and better estimates of detector performance, rather than replacement of
Initial LIGO scientific methods with new ones. We have demonstrated that the
optimized code reproduces the results obtaining in Initial LIGO, but at
substantially reduced computational cost. This gives us confidence that the
optimized code described here will be successful in Advanced LIGO as we
explore new methods that may further reduce the cost and allow exploration of
larger parameter spaces (in particular, the space of precessing binaries).

The rest of this document is organized as follows:
Section~\ref{sec:methodology} reviews the high-latency CBC search pipeline and 
Section~\ref{sec:MethodSpace} describes the  scientific methods that
we have investigated to implement the high-latency CBC search.
Section~\ref{sec:CompMethod} describes the
data-analysis methods that dominate the computational cost of the search
pipeline (the matched filter and the time-frequency signal-based veto).
Section~\ref{sec:CompAlgorithms} discusses the selection of optimal
algorithms, libraries, and tests of our implementation on CPU hardware. In particular,
Section~\ref{sec:opt-chisq} describes an improved implementation of the
time-frequency signal consistency test, and Section~\ref{sec:opt-thresh}
describes an improved algorithm for event finding and clustering in the
matched filter output. Both of these improvements are independent of hardware
implementation. We then focus on optimization on the LIGO reference CPU, the
Intel\textsuperscript{\textregistered} E5-2670 (which is similar to the E5-2680 used in Stampede).
Section~\ref{sec:opt-fft} describes the selection of optimal FFT engines for
this hardware, and Sections~\ref{sec:parall-expens-kern} and
\ref{sec:vect-expens-kern} describe the improvements to parallelize and
vectorize the non-FFT portions of the filtering code. With these improvements,
we find that the fastest FFT method (eight core multi-threaded FFTW) provides
the greatest search throughput. Section~\ref{sec:perf-relat-theor} compares the
best measured performance with our theoretical expectations.
Based on the fastest CPU implementation on the E5-2670, and taking into account instrument and astrophysics tuning, Section~\ref{sec:justification} calculates the
resources required for the production high-latency CBC searches, as summarized
in Table~\ref{t:offline-xsede-request}.   Finally,
Section~\ref{sec:offline-trade-study} describes our hardware trade study
investigating performance on available CPU systems and, in
Section~\ref{sec:gpu-trade}, our implementation of the high-latency search on
Graphics Processing Units.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\vspace*{-10pt}
 \section{Compact Binary Coalescence Searches} 
\vspace*{-5pt}
 \label{sec:methodology}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


If the angular momenta of the compact objects---their \emph{spins}---are
aligned with the orbital angular momentum of the binary (or the compact
objects are non-spinning), then the gravitational-wave strain $h$ observed by
LIGO or Virgo  (neglecting higher-order amplitude corrections) can be written as
\begin{equation}
h(t-t_c) = A(t - t_c) \cos( \phi(t - t_c) ) \cos \Phi - A(t - t_c) \sin( \phi(t - t_c) ) \sin \Phi,
\end{equation}
where $t_{c}$ is the coalescence time of the binary, $A \propto f_\mathrm{GW}^{2/3}$ is
the amplitude of the wave, $\Phi$ is a constant that depends on the
orientation of the binary, and $\phi$ is the time-evolving gravitational-wave
phase---the quantity to which LIGO and Virgo are most sensitive. The
gravitational-wave phase evolution is given by the particular waveform model
used in the search and depend only on the masses and spins of the compact
objects.  For detection of binaries with total mass $M \lesssim 12 M_\odot$ and spins $\chi
\lesssim 0.4$ (which includes binary neutron stars),  post-Newtonian theory
provides a sufficiently accurate analytic model of the gravitational
waveforms~\cite{Blanchet:2013haa,kidder:821,PhysRevD.47.R4183,Buonanno:2009zt,Brown:2012nn,Brown:2012qf}.
As the mass ratio and spins of the compact objects
increase~\cite{Brown:2012qf}, or the total mass of the binary
increases~\cite{Buonanno:2009zt,Brown:2012nn}, post-Newtonian waveforms 
become less accurate. In this case, we can model the signal
waveforms using analytic models methods, such as the effective one body (EOB)
approach~\cite{BuonannoDamour:1999} tuned to numerical relativity simulations of binary black holes~\cite{Taracchini:2013rva}, or by phenomenological models that capture the
dynamics of binary black holes~\cite{Hannam:2013oca,Purrer:2014fza}.

The amplitude of gravitational waves measured by the Advanced LIGO and Virgo detectors
is expected to be comparable to (or smaller than) the mean amplitude of the
detector noise.  Consequently, digital signal processing is required to
extract signals from the noisy detector data. For \ac{CBC} sources, it is
possible to construct models of the gravitational waveform. Matched
filtering~\cite{Wainstein:1962} would be the optimal method to identify
signals in the detector data, if the detector noise were stationary and
Gaussian. The noise in the LIGO and Virgo detectors from fundamental sources (thermal noise, radiation pressure noise, and photon shot noise) does behave in this way; however
populations of non-Gaussian, non-stationary transients of both environmental
and instrumental origin are also present. These ``glitches'' cause excursions in
the matched filter \ac{SNR} that may be mistaken for signals. Requiring that a
signal be seen in both LIGO detectors (and once running the Virgo detector) eliminates a substantial fraction of
false signals; however additional waveform consistency tests~\cite{Allen:2004gu}
are needed to determine if \ac{SNR} excursions are due to a glitch or a
gravitational wave~\cite{Babak:2012zx}. 
 

The gravitational waveform depends
sensitively on the mass and spin parameters of the source.  The parameters of
a signal are not known in advance, so 
a discrete ``bank'' of gravitational-waveform
templates~\cite{Owen:1998dk,Owen:1995tm,Balasubramanian:1995bm,Dhurandhar:1992mw,Sathyaprakash:1991mt} is constructed that is sensitive to the target astrophysical  population.
The computational cost of the search scales effectively linearly with the number
of templates in the bank, as the matched filter is applied once per template
in the bank. Template banks exist for binaries where the
angular momentum of the compact objects is negligible (non-spinning
binaries)~\cite{Cokelaer:2007kx,Babak:2006ty} and for the case in which the
component object's spin is aligned with the orbital angular momentum of the
binary~\cite{Brown:2012qf,Harry:2013tca}.
If the spin of the compact object is not aligned with the orbital angular
momentum (for example due to misalignment due to a supernova kick), spin-orbit
coupling will cause the plane of the binary to precess.  Spin-orbit and
spin-spin coupling also change the rate of energy and angular momentum loss
for the binary, which further changes the gravitational-wave signal.  
For binary neutron stars, a search for aligned-spin systems is sufficient to
capture precessing systems, as the effects of precession are not significant
for these systems~\cite{Brown:2012qf}.  However, for
binary black holes or neutron star--black hole binaries spin and precession
effects can be significant.  At present no template placement algorithm or
search pipeline has been implemented for spinning, precessing \ac{BBH} or \ac{NSBH}
binaries.  Search methods which incorporated spin effects were considered
in Initial LIGO, but were found to increase the false alarm rate 
resulting in a less sensitive
search~\cite{Pan:2003qt,Buonanno:2005pt,VanDenBroeck:2009gd}. Development of
precessing binary searches is an active topic of research, but these pipelines
are not yet in production for the LIGO-Virgo searches. In 
the absence of a search for precessing binaries, simulated signals from
a population of precessing binaries will allow us to quantify
the sensitivity of the LIGO-Virgo current search to the astrophysical population.

The full search for coalescing compact binaries requires: (i) generation of the
gravitational-wave template bank~\cite{Babak:2006ty,Brown:2012qf}; (ii) filtering
the data against this bank and identifying ``triggers'', or times where the
matched filter \ac{SNR} exceeds a certain threshold for a
particular template~\cite{Allen:2005fk}; (iii) checking triggers for consistency
using waveform consistency tests~\cite{Allen:2004gu}; (iv) folding in
information from instrumental health and status information to further
eliminate triggers due to instrumental
artifacts~\cite{Slutsky:2010ff,Aasi:2012wd-2}; (v) applying coincidence algorithms
to ensure that a gravitational-wave signal is present in two or more detectors
with consistent signal parameters; and (vi) measuring the significance of a
candidate signals by comparing their amplitude to that of the noise-induced
background.   Executing all of the above steps is the job of the \emph{analysis
pipeline}~\cite{Babak:2012zx,Brown:workflow}, a program that generates a
workflow that turns the raw detector data into a detection statement or
measures the signal parameters. The search pipeline is a heterogeneous mixture
of computational components that perform the required steps on all of the data
in the correct order. LIGO-specific scripts write workflows that are planned
by the Pegasus Workflow Management System into directed acyclic graphs (DAGs) that are executed by
HTCondor's DAGman. 
The computational cost of the full pipeline is dominated by the executable that
computes the matched filter and the waveform consistency tests. Based on a test
filtering 11.5 days of simulated Advanced LIGO data, we find that 99.8\% of the total 3727.63 core-days of runtime is spent in the \texttt{pycbc\_inspiral} filtering engine (that computes the templates, the correlation, the FFT, and event finding and clustering), with the remainder spent in the
coincidence, background estimation, and post-processing steps. Consequently,
we have focused our optimization efforts to date on the filtering executable
(although improvements have also been made to other parts of the pipeline).

A complication encountered when
benchmarking the offline CBC search is that the run time of the search  depends on the quality
of the (random) detector noise. To save computational cost, the filtering
executable only computes waveform consistency checks when the \ac{SNR} exceeds
a threshold value. However, we do not know in advance the character or the
rate of non-stationary noise transients in the data. Indeed, this rate can
change over time during an observing run.  To compute the computational cost
of the analysis, we take the average of the measured throughput over three
representative types of data. 

% Investigation of early
%Advanced LIGO data from the Livingston detector indicates that this is a
%reasonable assumption for the (as yet unknown) quality of data in the first
%Advanced LIGO observing run. 

% FIXME
%\todo{is it possible, and interesting, to put a number to the consequences for very good (Gaussian) noise performance or very poor (really glitchy) performance? is this by any chance a potential need for shared resources?}\todo{dhs more generally, need paragraphs here and there, and also in the Exec Summ, that acknowledge the uncertainty in the computing costs due to the instrument performance: both tuning choices, AND data quality. We want the NSF to be primed to hear that as a reason for potentially large changes in the total request.}

%To account for this, we benchmark the code
%on three types of data (labeled A, B, and C) from Initial LIGO, which are
%representative of different types of data quality: (i) a clean stretch of data
%(Type A); (ii) a stretch containing a single loud transient glitch (Type B);
%and (iii) a stretch with elevated levels of non-Gaussian noise at low
%frequencies (Type C).  Type C is the worst type of data quality in terms of
%computational cost. The speed of the analysis is significantly slower for this
%type of data, as more time is spent computing signal based vetoes. 



%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\section{Exploring the space of appropriate scientific methods}
\vspace*{-5pt}
\label{sec:MethodSpace}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The scientific methods used in the pipeline may be implemented by more than one computational
algorithm. For example, the matched filter signal-to noise ratio for a bank of
templates may be constructed by a frequency-domain correlation or linear
combinations of time domain correlations with an orthonormal filter set found
via the singular value decomposition (SVD).  Benchmarking of the LLOID
algorithm~\cite{Cannon:2011vi} (which uses SVD and muti-rate filtering to implement a low-latency
search as described in LIGO-T1400542) versus the frequency-domain FFT~\cite{Allen:2005fk}
has shown that the FFT method has a higher
template-per-core throughput for a given input data sample rate.   We have
therefore selected the frequency-domain FFT correlation as the optimal
scientific method to search for signals with the deep, offline pipeline where
latency is not a concern. Since the LLOID method is used to search for signals with a
possible rapid electromagnetic counterpart, it is appropriate to trade
computational cost for latency in the low-latency search.

Non-Gaussian noise transients in the detector data can cause the matched filter
to generate false triggers, and so a variety of \emph{signal-based vetoes}
have been developed that use additional information to distinguish signals
from noise. These are often called $\chi^2$-vetoes, as the three primary tests
considered (known as the time-frequency signal-based veto, the autocorrelation
signal-based veto, and the template bank veto) all generate a statistic that is
$\chi^2$ distributed in Gaussian noise. The low-latency CBC search implements
the autocorrelation signal-based veto, as it is straightforward to compute in
the low-latency search given its locality in time and dependence only on the
SNR time-series.
The offline search used in Initial LIGO
CBC searches (and used for cost estimates in the May 2014 review) computed the
time-frequency $\chi^2$ veto and the bank veto. The time-frequency $\chi^2$ veto
has been demonstrated to be a powerful test and is essential to eliminate
non-Gaussian transients from the search.  However, it has not been
demonstrated that the bank veto provides additional noise rejection power
beyond the time-frequency $\chi^2$ veto.  It has therefore been decided to
eliminate the computation of the bank veto from the filtering to save
computational cost.
We are also exploring the use of the autocorrelation $\chi^2$ veto (used in
the low-latency search) in the deep, offline search. Preliminary
investigations with prototype code suggest that this may provide additional information to the
time-frequency $\chi^2$ test for certain types of noise events, however it is
not yet clear if this veto provides more noise-rejection power than the standard
time-frequency $\chi^2$.

Two further refinements of the  scientific methods and tuning used in Initial
LIGO are being explored: (i)  In Initial LIGO, the search algorithm processed
fifteen data segments of length 256 seconds though each template, with noise
power spectral density (PSD) estimation performed over 2048 seconds. The new
\texttt{pycbc\_inspiral} executable allows us more control over the PSD
estimation, as well as the number and length of the filter data segments.
Preliminary investigations with increasing the length of the data segment to
512 seconds have shown a performance increase of $\sim 25\%$, as longer data
segments allow us to make more efficient use of the matched filter output by
decreasing the amount of time per data segment corrupted by the wrap-around
of the FFT; (ii) Re-using the template for more data segments also further
reduces the overall cost of the code. If Advanced LIGO provides sufficient
long, stable lock stretches, we can increase the number of data segments per
template, further reducing the overall search cost.

Moving beyond the existing methods, we are developing a new
matched-filtering algorithm that performs a search that is hierarchical in
sample rate.  An approximate signal-to-noise time series is first created by
reducing the sample rate, allowing the use of shorter, faster FFTs. Peaks of
interest are found using a threshold that has been lowered to account for both
the loss of the high frequency contribution to the SNR and the time offset
from the full sample rate peak.  Full sample rate matched filtering is
performed at these peaks and the nearby points, to minimize the probability
that the full sample rate peak is missed.  This step is accelerated using a
pruned FFT algorithm, where we decompose an N points FFT into a batched set of
FFTs  each $N^{(1/2)}$ in length, followed by an explicit DFT calculated for
every point. A further optimization removes the first memory transposition of
the FFT by storing the input data in a memory layout that is already
transposed for the first batched FFT. This algorithm is efficient due to the
small ratio between the number of interesting points to calculate and the number of points in
the full time series, $O(10^{-4})$, which allows us to make more efficient use
of the Level 3 cache.

This method performs an accurate approximation to the full matched filter;
however, it does not exactly reproduce the same output. Consequently, this
method require careful testing before it can be commissioned as a production
search. % to quantify any possible loss in event rate due to the hierarchical
%algorithm. 
A prototype implementation has been tested in the binary black
hole search, with the initial version of the code showing a 2--3 times speedup over the full
sample-rate computation of the matched filter. Ongoing work to develop this
method includes
increasing the efficiency of the full sample rate reconstruction which will
allow this algorithm to be efficient in the widest parameter space range. If
this new method can be demonstrated to yield the same detection efficiency as
the current methods, we will adopt it for Advanced LIGO, further reducing
computational cost.


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\vspace*{-10pt}
\section{Computational Methods}
\vspace*{-5pt}
\label{sec:CompMethod}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The computational cost of the CBC search is dominated by the
\textsc{findchirp} matched filtering algorithm~\cite{Allen:2005fk} that
computes the matched-filter SNR and the waveform consistency test for a single
detector; combining these triggers from multiple detectors is relatively
inexpensive.
The matched-filter \ac{SNR} $\rho^2$ for the data $s$ and template $h$, analytically
maximized over $A$ and $\Phi$, is given by
%
\begin{equation}
\label{eq:cbc_snr}
\rho^2 = \frac{ (s|h_0)^2 + (s | h_{\pi/2})^2 }{(h_0|h_0)}; \quad
\textrm{with} \quad
(a|b) = 4 \, \mathrm{Re} \int_{f_{\mathrm{low}}}^{f_{\mathrm{high}}}
\frac{\tilde{a}(f) \, \tilde{b}^{*}(f)}{S_n(f)} df,
\end{equation}
%
where $S_n(f)$ the one-sided detector-noise \ac{PSD} and $h_0$ and $h_{\pi/2}$
correspond to the two gravitational-wave polarizations. 
If a gravitational wave signal is present in the data, then its location in time is
defined by the parameter $t_c$. To search over all possible
times $t_c$, we use a Fast Fourier Transform to compute the value of the inner product $(s|h_0)$ by
\begin{equation}
(s|h_0(t_c)) = 2 \int_{-\infty}^\infty\,df e^{2\pi ift_c}
\frac{\tilde{s}(f)\tilde{h}_0^\ast(f)}{S_n(|f|)}
\label{eq:ipift}
\end{equation}
and the square of the \ac{SNR} for a chirp that ends at time $t$ is
\begin{equation}
\rho^2(t) =  \frac{1}{(h_0|h_0)} \left[(s|h_0(t))^2 + (s|h_{\pi/2}(t))^2\right] \equiv \frac{1}{\sigma^2}\left[ \rho_0^2(t) + \rho^2_{\pi/2}(t)^2  \right]
\end{equation}
where the two \ac{SNR} time series $\rho^2_0(t)$ and $\rho^2_{\pi/2}(t)$ can be obtained by
inverse Fourier transforms of the form in Eq.~(\ref{eq:ipift}).
 The \textsc{findchirp} algorithm incorporates
several optimizations to compute the matched-filter SNR.
\textsc{findchirp}  assumes that the two chirp
waveforms $\tilde{h_0}$ and $\tilde{h}_{\pi/2}$ are orthogonal. This is
identically true for the aligned-spin waveforms used in \ac{BNS} and \ac{NSBH} searches,
and approximately true for the slowly-evolving inspiral part of \ac{BBH} waveforms.
The filtering cost is reduced by 
packing the two filter phases into
the real and imaginary components of a single complex inverse FFT rather than
computing it independently from two real inverse
FFTs. For BNS
and NSBH waveforms, we use the stationary phase approximation to write the
waveform directly in the frequency domain~\cite{Droz:1999qx}, eliminating the
cost of Fourier transforming the waveform.  \textsc{findchirp} further
increases efficiency when using frequency-domain templates by splitting the
filter into a part that depends on the data and a part that depends only on
the template parameters, and reuses a template for several (typically 15) data
segments before generating the next template.  These optimizations further
reduce the cost of computing the integrand of the matched filter~\cite{Allen:2005fk}. 
The computational cost of the matched filter is therefore dominated by the
complex vector multiplication needed to compute the integrand of Eq.~(\ref{eq:ipift}) and the complex inverse FFT used to compute the SNR as a function of signal arrival time. A further step to find and cluster peaks in the SNR time series is computationally cheap, but may may be a performance bottleneck if not implemented carefully, particularly if multi-core FFTs are used to compute the matched filter.

Non-Gaussian noise transients in the detector may cause
high SNR excursions which the matched filter alone cannot distinguish from
signals. To distinguish a high \ac{SNR} due to a
signal from one due to a glitch, we use a time-frequency signal-based veto known as the time-frequency signal-based
$\chi^2$ veto~\cite{Allen:2004gu}. This test divides 
the two template phases $h_0$ and $h_{\pi/2}$ 
into $p$ frequency sub-intervals $\{h_0^l\}$ and $\{h_{\pi/2}^l\}$, $l=1\ldots p$ with
\begin{equation}
  (h_0^l|h_0^m) = \frac{1}{p}\delta_{lm}, \ 
  (h_{\pi/2}^l|h_{\pi/2}^m) = \frac{1}{p}\delta_{lm}, \ 
  (h_0^l|h_{\pi/2}^m) = 0
\end{equation}
and $h_0=\sum_{l=1}^p h_0^l$ and $h_{\pi/2}=\sum_{l=1}^p h_{\pi/2}^l$. We can
then construct the $2p$ quantities $\{\rho_0^l\}  =(s|h_0^l)$ and $\{\rho_{\pi/2}^l\} = (s|h_{\pi/2}^l)$, where 
where $s$ is the detector output. The $\chi^2$ test is constructed by
computing
\begin{equation}
\chi^2 = p \sum_{l=1}^p \left[ (\Delta x_l)^2 + (\Delta y_l)^2 \right]
\quad\textrm{where}\quad
\Delta x_l = \rho^l_0 - \frac{\rho_0}{p} \textrm{ and }
\Delta y_l = \rho_{\pi/2}^l - \frac{\rho_{\pi/2}}{p}.
\end{equation}
In the presence of Gaussian noise $s=n$ this statistic is $\chi^2$ distributed
with $\nu=2p-2$ degrees of freedom.  Furthermore, if a signal is present along with
Gaussian noise $s=h+n$, then $\chi^2=pr^2$ is still $\chi^2$ distributed
with $\nu=2p-2$ degrees of freedom. Small values of the $\chi^2$ veto mean
that the \ac{SNR} has been accumulated in a manner consistent
with an inspiral signal. Since the value of the $\chi^2$-veto is only computed
when peaks in the \ac{SNR} are detected, the total computational cost depends
on the noise content of the input data, which is non-deterministic. 


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\vspace*{-10pt}
\section{Identifying computational algorithms that efficiently implement
    the scientific methods}
\vspace*{-5pt}
\label{sec:CompAlgorithms}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this section, we consider the optimizations that we have made to 
to the scientific methods selected.  In
Initial LIGO, searches were performed with the \textit{ihope} search pipeline.
The ihope pipeline was developed over the six initial LIGO science runs based
on our experience searching kilometer-scale interferometric detector data for
the first time. To provide a computational cost estimate for the May 2014 NSF
review, we analyzed two weeks of Initial LIGO/Virgo data with the ihope search
pipeline, in the configuration that was most recently used in the the
S6/VSR2,3 science run~\cite{Babak:2012zx,Colaboration:2011np}. The dominant
computational cost of the ihope pipeline is the \texttt{lalapps\_inspiral}
filtering engine. Over the last two years, the ihope pipeline has been
re-written for Advanced LIGO. The new framework, known as PyCBC is more modular,
flexible, and scalable than the LALApps framework used previously. PyCBC has
been developed to accommodate longer templates and larger template banks
necessitated by the improved detector noise profile, as well as the lessons
learned from the May 2014 NSF review and our optimization experience over the
last year.

The PyCBC architecture implements the high-level program control in Python,
however computations are performed using C code compiled just-in-time by the
\texttt{scipy.weave} framework~\cite{scipy}.  This ensures that all computationally intensive parts
of the pipeline are executed by low-level, optimized code and not by the
Python interpreter. Furthermore, direct AVX/SSE calls or OpenMP parallelization
may be performed by use of the X86 intrinsic functions in the weave-compiled
C-code.  The Python frame work allow us to modularize the low-level kernels at
low overhead. It is therefore straightforward to replace these kernels with
code for new compute architectures including Graphics Processing Units (GPUs)
and Intel\textsuperscript{\textregistered} MICs (in addition to architecture-specific CPU code) in the same
search engine. This modularization reduces the human cost of development,
validation, and verification, which is a concern given the small size of the
development team (approximately 4 FTEs).

As a result of this development, the the \texttt{lalapps\_inspiral} filtering
engine has been retired and replaced with the new \texttt{pycbc\_inspiral}
executable. Our computational costs for Advanced LIGO are computed with the
best current version of \texttt{pycbc\_inspiral} on CPUs and GPUs; however, we
provide some benchmarking for \texttt{lalapps\_inspiral} in
Appendix~\ref{a:lalapps} to illustrate
changes between the current code and the numbers presented in May 2014. 
In the
sections below, we review the optimizations that have been made to the
CPU-based code to obtain our current performance numbers. 


%~~~~~~~~~~~~~~~
\vspace*{-10pt}
\subsection{Algorithmic Optimizations}
\vspace*{-5pt}
%~~~~~~~~~~~~~~~

In this section, we discuss improved algorithms that implement the selected
scientific methods. By refactoring the code used to implement the
time-frequency $\chi^2$ signal-consistency test and the event finding and
clustering, we have made performance improvements that can be realized
independent of the architecture used (CPU or GPU). The improved algorithms
generate \emph{exactly the same output} as used in previous LIGO searches, as
compared to a different choice of scientific method which may implement a
somewhat different search (e.g. the hierarchical methods discussed above).

\vspace*{-10pt}
\subsubsection{Optimization of the $\chi^2$ signal-consistency test}
\vspace*{-05pt}
\label{sec:opt-chisq}

If points are found above threshold in the matched filter signal-to-noise time
series, then the a time-frequency signal consistency test is applied.  The test
consists of breaking the waveform into $p$ frequency bins of equal power. Each
bin is filtered against the data to obtain the partial SNR contribution
$\rho_l$ and then compared to the expected SNR contribution $\rho / p$. This
is expressed as
\begin{equation}
 \chi^2 = p \sum_{l=0}^{p} [\rho_l - \rho / p]^2,
\end{equation}
The calculation of each bin $p$ requires a single FFT, and
neglecting lower order terms, we find a cost of
\begin{equation}
\textrm{FLOP} = p \times 5N \log(N).
\end{equation}
The \texttt{lalapps\_inspiral} implementation of this test computed the $\chi^2$
time series for the \emph{entire} data segment, if any points in the matched
filter time series exceeded the threshold. This is computationally efficient, if
there are many threshold crossings. However, if only a few points cross the
threshold, then computation is wasted computing the $\chi^2$ veto at
unnecessary times. As we know the location of peaks in the SNR time series, we
can directly calculate the $\chi^2$ test only for those points. We can express
the quantity that needs to calculated in terms of existing information as,
\begin{equation}
\frac{\chi^2 + \rho^2}{p}[j] = \sum_{l=0}^{p}\rho_l^2
\end{equation}
We can write this in terms of the quantities computed by the
\textsc{Findchirp} matched filter as
\begin{equation}
\frac{\chi^2 + \rho^2}{p}[j] = \sum_{l=0}^{p}\left(\sum_{k=k^\mathrm{min}_l}^{k^\mathrm{max}_l}\tilde{q}_k e^{-2\pi i jk/N}\right)^2
\label{eq:pointchisq}
\end{equation}
where $\tilde{q}_k$ is the kernel of the matched filter (the frequency domain
correlation of the noise-weighted template with the data), and the index $k$
runs over frequency bins (typically $k_\mathrm{max} - k_\mathrm{min} \sim
10^5$). The quantity $[j]$ is the set of indices of the $N_p$ peak values of the matched filter SNR. We note that the fact that $\tilde{q}_k$  is required in Eq.~\ref{eq:pointchisq} is the reason that we use out-of-place FFTs for the matched filter.
Computing Eq.~\ref{eq:pointchisq}  involves
the calculation explicitly of $k_\mathrm{max} \sim 10^5$ twiddle factors\footnote{The trigonometric constant coefficients that multiply the data.}. This can be reduced to a complex
multiply by calculating a single twiddle factor and iteratively finding the next factor, i.e.
\begin{equation}
\frac{\chi^2 + \rho^2}{p}[j] = \sum_{l=0}^{p}\left(\sum_{k=k^{min}_l}^{k^{max}_l}\tilde{q}_k (e^{-2\pi ij/N}) (e^{-2\pi ijk/N})^{k-1}\right)^2                                        
\end{equation}
This reduces the computation cost to two complex multiples, one for the twiddle factor calculation,
and one for the multiplication by $\tilde{q}$, along with a add of two complex numbers giving,
\begin{equation}
 \textrm{FLOP} = 14 \times k_\mathrm{max} \times N_p
\end{equation}
For small values of $N_p$ we note that this can be vastly more efficient than the full FFT based
calculation of the veto. The crossover point can be estimated as,
\begin{equation}
N_p = \frac{p \times 5N\log(N)}{14 N_p k_\mathrm{max}}.
\end{equation}
This algorithm has been implemented in the \texttt{pycbc\_inspiral} filtering
engine. The
number of SNR threshold crossings is computed and the full $\chi^2$ time
series is calculated only if this number exceeds the above crossover point.
Otherwise the $\chi^2$ veto is computed in a point-wise manner at reduced
cost. The exact cost reduction depends on the quality of the data, but on
average, application of the approach on Initial LIGO data gives a reduction in computational cost of approximately a factor
of four.

\vspace*{-10pt}
\subsubsection{Optimization of thresholding and time clustering}
\vspace*{-05pt}
\label{sec:opt-thresh}

After the matched filter SNR is computed for a given template, 
the resulting time
series must be searched for points above a runtime-specified threshold to
obtain gravitational-wave candidate triggers.  Since both signals and glitches can produce
many nearby SNR samples above threshold (which do not represent independent
triggers), the SNR samples above threshold tend to be clustered in time. This leads to a high probability
that there is a minimum spacing of a user-specified length (the
clustering window) between any two consecutive clustered triggers. This window
is chosen based on the impulse response of the filter and the character of the
data, so that triggers produced come from independent events (noise or signal).

In \texttt{lalapps\_inspiral} these two steps (thresholding and clustering) were
implemented as separate kernels; this optimization fuses them into one.  The
primary motivation for this fusion is the thresholding step.  Searching through
an array for points above threshold is trivial to implement in serial,
un-vectorized code.  Vectorization or
parallelization of this code must be done with care; the problem is 
equivalent to \emph{stream compaction}, which is difficult to vectorize or
parallelize without requiring at least two passes over the array to be compacted
\cite{Billeter:2009:ESC:1572769.1572795}. However, the number of floating point
computations to be performed for each memory operation is very low, and so this
kernel will be bandwidth limited; multiple passes over the array incur heavy
performance penalties. The primary difficulty is that stream compaction takes
its input array and writes out another array consisting of all elements of the
input satisfying some criterion, consecutively.  This cannot be vectorized or
parallelized in one step, because the location to which the output should be
written potentially depends on the calculation of all input array elements
before any given element.

Fusing the array compaction and the clustering allows us to bypass this
difficulty. The key idea is to find the maximum of the output over sub-arrays no
longer than the clustering window, and write one output for each such window.
We can do this in a single pass over the data, since the output destination is
predetermined.  We then cluster in a followup pass that looks at the maximum
for each window.  While that followup pass is not parallelized, in our typical
configurations it looks at of order one hundred array elements, rather than a
million, and so has trivial cost in comparison. This change greatly
improves the performance of both CPU and GPU implementations, and the CPU
particularly when multi-threaded FFTs are used to compute the matched filter. 

%~~~~~~~~~~~
\vspace*{-10pt}
\subsection{CPU implementation and optimization}\label{sec:cpu-opt}
\vspace*{-05pt}
%~~~~~~~~~~~

We now turn to the specific optimizations and implementation choices necessary
for CPU architectures.  For concreteness, we focus on the
Intel\textsuperscript{\textregistered}~E5-2670 (Sandy Bridge) product, which is
nearly identical (except for slightly lower clock speed) to the cores on
Stampede. 
Our testing included standardized performance tests, employed for all the
LSC optimization characterization, with \texttt{perf-stat} results given in
Section~\ref{sec:measured}, both below.
Similar to Stampede, our reference system
has two sockets of eight cores each, running at 2.6~GHz clock speed. 
All performance results presented here, whether single or multi-threaded, were
tested with the CPU affinity of the process set to bind it to a number of cores
equal to the number of threads assigned to that process, and resident on the
same CPU socket.  CPU throttling and hyper-threading were also disabled for
these tests.
 Each
socket has a unified shared L3 cache of 20~MB, and each core has an L1 data
cache of 32~KB, and an L2 cache of 256~KB. The architecture supports the AVX
(but not AVX2) instruction set, and each core therefore has access to sixteen
SIMD registers that can hold either eight single-precision or four
double-precision floating point numbers. Potentially one add and one multiply
instruction can be retired each clock cycle, so the maximum theoretical peak
single precision performance of each socket is $2.6\times 8\times 8\times 2 =
332.8~\mathrm{GFLOPS}$. We have tested our code on other CPU architectures as
reported in the trade study in Section~\ref{sec:offline-trade-study}; in the following subsections we focus only on the
E5-2670. Similar considerations, though with potentially different details,
would apply to other CPU architectures that are or might be available to the
LSC.

Standard profiling tools can reveal where \texttt{pycbc\_inspiral} spends most of
its time, and timing tests can reveal whether we are in fact able to utilize the
most efficient, multi-threaded FFT.  Initially, that configuration did
\emph{not} give us the highest throughput per socket: the other kernels in the
core matched filter were not well parallelized or vectorized and though their
cost was small when the program was run in a single-threaded configuration, they
became unacceptably slow when the FFT was switched to the faster, multi-threaded
configuration. Indeed these kernels before and after the FFT were sufficiently
slow in their original implementation that not only did we not achieve close to
the matched filter performance expected based on the FFT alone, we did not
achieve the highest throughput by running in a multi-threaded
configuration. We therefore began our CPU optimization by both vectorizing and
parallelizing these kernels, and in the next subsections we report in some
detail on those changes, and the resulting performance improvements.

One expensive kernel remains that has not yet received a thorough optimization
in its CPU implementation: the time-frequency $\chi^2$ veto.  This kernel is more complex and
is also only a significant bottleneck when the data quality is poor enough that
there are many candidate triggers per segment above threshold. % that must be
%followed up. In S5 and S6 these periods of very poor data quality were
%comparatively rare; \todo{can we quantify? reference?} nevertheless, 
Our next
optimization target is a careful vectorization and parallelization of this
algorithm. %, both in case O1 data should turn out not as clean as we might hope,
%and for the further improvement in performance it will bring even in relatively
%clean data periods.
If the
autocorrelation $\chi^2$ veto is also
shown to be necessary, we
will also implement an optimized kernel for the algorithm. 


\vspace*{-10pt}
\subsubsection{Selecting the optimal FFT library}
\vspace*{-05pt}
\label{sec:opt-fft}

By design, the count of floating point operations in the basic matched filtering
step that compares detector data to a template is dominated by the operation
count of the Fast Fourier transform, since that scales as $N\log{N}$ while other
steps scale linearly (or less) in the data segment size. A properly implemented
\textsc{findchirp} executable should therefore likewise have its running time
dominated by the FFT, and that FFT should be performed using the most efficient
available library.

We have tested two modern, efficient FFT libraries: the Intel Math Kernel
Library (MKL) and the Fastest Fourier Transform in the West (FFTW)
\cite{FFTW05}. To make optimal use of these libraries we ensure that all memory 
provided to FFT calls is 32-byte aligned, and for FFTW
that SSE, AVX, and parallelization are enabled within the
library. For \textsc{findchirp}, we must use an out-of-place transform, because
the input vector to the FFT (the result of correlating the template with the
data) must be preserved in case any points above the runtime-specified threshold
are found, as the $\chi^2$ test will require that same input vector.

Finally, because our data analysis pipeline is embarrassingly parallel, there
are multiple \textit{a priori} plausible methods of utilizing the multiple cores
of the hardware. The legacy \texttt{lalapps\_inspiral} program was only capable
of doing so by running multiple, single-threaded instances, but
\texttt{pycbc\_inspiral} can run multi-threaded. In the standard configuration,
either executable performs an inverse, complex-to-complex single-precision
out-of-place FFT of length $2^{20}$. The input and output to this inverse FFT
together require 16~MB of storage, which fits within the L3 cache of a single
socket. However, if multiple single-threaded FFTs are performed, they each
require this amount of memory but must share the same 20~MB L3 cache; thus, the
competing single-threaded processes will be frequently evicting one another from
cache and the overall throughput should be expected to decline.

\begin{table}
  \centering
  \begin{tabular}{|c|c|r|r|}\hline
   \textbf{FFT library} & \textbf{Thread configuration}& \textbf{$2^{19}$ length}& \textbf{$2^{20}$ length} \\ \hline
    MKL & 8 single-threaded & 10400 $\pm$ 130 & 23500 $\pm$ 260  \\ \hline
    MKL & 1 eight-threaded & 515 $\pm$ 9  & 2120 $\pm$ 60   \\ \hline
    FFTW & 8 single-threaded & 7640 $\pm$ 510 & 20700 $\pm$ 560 \\ \hline
    FFTW & 1 eight-threaded &  432 $\pm$ 4 & 1100 $\pm$ 37   \\ \hline
  \end{tabular}
  \caption{Time (in $\mu$s) to perform an FFT on E5-2670, per invocation. Smaller
numbers represent better performance. The 8 thread OpenMP parallel FFTW configuration is
the best performing FFT configuration for both transform sizes.}
  \label{tab:fft}
\end{table}

This is indeed what we find; in table~\ref{tab:fft} we see that eight
single-threaded FFTs (for either MKL or FFTW) each require much more than eight
times as long as an eight-threaded FFT; the ratio of single-threaded to eight
threaded execution time varies from 11 to 20, depending on the transform size and
library. But by far the highest throughput for the $2^{20}$ size FFT is an
eight-threaded FFTW implementation, so we wish to design the rest of our
executable so that this implementation also retains the highest throughput for
\texttt{pycbc\_inspiral}. 

\vspace*{-10pt}
\subsubsection{Parallelization of expensive kernels}
\vspace*{-05pt}
\label{sec:parall-expens-kern}

Both the correlation of the frequency-domain data segment with the frequency
domain template (to produce the input to the inverse FFT) and the combined
thresholding and clustering algorithm (described in
subsection~\ref{sec:opt-thresh} above, and acting on the output of the inverse
FFT) are implemented in the pipeline as C-code kernels.  These are parallelized
with OpenMP and will dynamically adjust to run on all cores made available to
the kernel. The optimal performance was achieved not by a straightforward
\texttt{for} loop parallelization, but rather by parallelizing a loop that
called another function to act on ``chunks'' of data, where the chunk size is
chosen to maximize the amount of data that can fit in the L2 cache of each
core.

The quality of parallelization is relatively easy to quantify: a given kernel is
benchmarked running on a single core with all other cores idle, and that
benchmark compared to the kernel executing on all cores of the socket. Again, we
reiterate that we always set the CPU affinity of a kernel so that the operating
system cannot dynamically migrate it. If the parallelization is optimal, then
the ratio of the single-threaded execution to multi threaded should be the number
of cores on the socket, in our case eight.

For correlation of the first half of two arrays of length $2^{20}$ with output
written to a third such array, the parallelized kernel executed on all eight
cores in a time of $87.2\,\mu\mathrm{s}$; the single-threaded kernel in
$581\,\mu\mathrm{s}$, for a ratio of 6.7.  For the combined
threshold-and-cluster kernel, the eight-threaded kernel executed in
$69.3\,\mu\mathrm{s}$, and the single-threaded in $379\,\mu\mathrm{s}$, for a
ratio of $5.5$.  While these ratios are not quite at 8, as we would desire, they
are still sufficiently close that they do not affect by themselves the
performance of the FFT greatly: he difference between the observed
multi-threaded performance and the theoretical performance that perfect scaling
would imply is of order $35\,\mu\mathrm{s}$ combined, or roughly 4\% of the
execution time of the optimal FFT. As described below, other cache effects
dominate over this, but when this becomes a bottleneck we will again investigate
improving it further.

\vspace*{-10pt}
\subsubsection{Vectorization of expensive kernels}
\vspace*{-05pt}
\label{sec:vect-expens-kern}

The C implementation of the correlation and thresholding has also been vectorized to
support SSE4.1 and AVX. 
%These instructions are used when they are available, and the code fall backs to a standard
%C implementation when not they are not. 
The vectorization is hand-coded using compiler
provided instrinsic functions that map directly onto SIMD instructions, and the
loops are unrolled to permit the vectorized kernel to operate on an entire cache
line. Wherever possible memory loads and stores are performed with the
``aligned'' memory intrinsics, and the arrays on which these kernels act are
allocated with 32-byte aligned memory, as they are for the FFT call. Much as for
parallelization, for the fused threshold-and-cluster kernel, an efficient
vectorization is only possible because of the algorithmic change summarized in
section~\ref{sec:opt-thresh}. 

As a first estimate of the quality of vectorization, we can benchmark this
kernel in isolation and see how many of their instructions are indeed packed
AVX instructions; for threshold, this was 99.6\%, and for correlate, 100\%. Thus
the compiler is indeed generating exclusively AVX instructions as we have
directed it to via the intrinsic functions.
We can quantify the quality of the vectorization similarly to our
quantification of the parallelization: benchmarking the kernel with it on and
off. In our case it is relatively straightforward to disable most of the
vectorization; though it has been hand-coded with vector intrinsics, these are
always wrapped in preprocessor directives to allow a graceful fall-back to
straight C-code.  Hence the intrinsics can be commented out and compiler flags
given to prevent the compiler from generating most such instructions on its
own\footnote{It is not possible to prevent \emph{all} SIMD instructions; because
  the operating system is 64-bit, the C-library is compiled with a minimal set
  of SSE instructions, so that turning off all SIMD instructions generates linking
  errors.}.
This comparison has been made for both the correlation and thresholding and
clustering kernels, where the ratios are 1.83 and 2.34, respectively. 

At first sight these ratios appear quite poor, since for the Sandy Bridge AVX
instruction set, the peak theoretical speedup from 
vectorization is a factor of sixteen for single precision code.  That factor
comes from a factor of eight for the SIMD single-precision vector width and
another factor of two because the core can generate a multiply and an add at
each clock cycle.  Of course, achieving this peak theoretical speedup is often
difficult in practice: the latencies of the multiply and add instructions are
five and three clock cycles, respectively, and there are only sixteen SIMD
registers that can serve as operands for these instructions. Thus only very
specific problems will have the necessary data independence and structure to
allow retiring 16 single-precision SIMD arithmetic operations per clock cycle.

Our kernels do not have such structure.  The correlate kernel is simpler to
analyze, since it is almost identical to element-by-element complex
multiplication, for which AVX optimized code is widely available (including from
Intel). The only difference between our code and these is that we must add a
single instruction, to complex conjugate one of the input vectors. A standard
single-precision complex multiplication requires six floating point operations
(four multiplications and two additions); an AVX register can hold four single
precision complex numbers. Thus the relevant speedup would be how many clock
cycles are required to execute the AVX multiplication of the 24 floating point
operations equivalent to the multiplication of four complex numbers
simultaneously. Because of the need to conjugate an operand as well as the
shuffle operations inherent to complex multiplication, there are seven
instructions needed for this calculation (there are six in the widely available
libraries for AVX complex multiplication; our modification to calculate the
complex conjugate adds only a single instruction with a latency of one clock
cycle),  giving a theoretical speedup of a factor of $2\times (24/7) = 6.86$, if
we were in fact able to retire two AVX instructions per clock cycle. The
analysis of the thresholding and clustering algorithm is similar if more
complex; each execution of the inner loop requires eight AVX instructions to
find the location and values of the maximum of four consecutive complex numbers,
which corresponds to 16 scalar floating point operations if we include the
comparison. Thus the maximum speedup is only a factor of four, at most. 

The further gap between the theoretical peak speedup of vectorization and our measurement can be attributed to memory bandwidth. The correlation kernel
reads in two single precision complex numbers---equivalent to four single
precision floating point numbers---and writes out a third; between these memory
operations, it performs six floating point computations (four multiplies and two
adds). There is therefore a one-to-one ratio of memory operations to floating
point operations.  For the threshold and cluster kernel, two floats are read,
and three floating point operations performed, for a floating point to memory
ratio of 1.5. The low floating point to memory ratios mean that any kernel
implementing them will be memory bandwidth bound.

We can compare the execution times of these kernels to what memory
bandwidth-limited kernels could perform.  A correlation for a $2^{20}$ FFT
length must read two vectors of half that length (because the second half is
always zero, as part of the \textsc{findchirp} algorithm to maximize over
unknown inspiral phase) and write out a third vector of half that length; a
total of 12~MB of memory transactions must occur. If all of that memory lived in
the computer's RAM, then we can measure its bandwidth using the STREAM
benchmark~\cite{McCalpin1995}; for a single socket this bandwidth is
approximately\footnote{It is possible to improve this by roughly a third
  by forcing the use of \emph{streaming stores}; however, while this
  significantly improves the bandwidth as measured by STREAM, it does so by
  bypassing the cache on writes.  Since the only kernel with significant writes
  is correlation, this is not beneficial: the output of the correlation
  \emph{needs} to remain in cache if possible since it will immediately become
  the input to the FFT.} 26~GB/s. For correlation, this would imply an execution
time of 460~$\mu$s, much higher than what is measured, and 307~$\mu$s for
thresholding, again much higher than observed.  

That is unsurprising, since we want the data for those calculations to remain in
cache and the benchmark performance numbers for those kernels reflect a repeated
execution from within cache.  Our kernels are parallelized with the goal that
each ``chunk'' remains in L2 cache, which has a published latency of 12
cycles~\cite{inteloptimize.2014.09}.  However since our memory for each kernel
is accessed sequentially we expect that hardware prefetching ensures that the
next data to be read is almost always in the L1D cache, which has a \emph{load}
latency of typically five cycles, though it can be as high as seven cycles for
AVX loads. For an eight-core E5-2670, which can load or store up to 32 bytes per
core, these latencies and the 2.6~GHz clock speed imply an effective load
bandwidth of 95 to 133~GB/s.  The 87~$\mu$s execution of the correlate kernel (which
must move 12~MB of memory) would correspond to a bandwidth of 138~GB/s, and the
69~$\mu$s execution of the threshold and cluster kernel (which reads 8~MB of
memory) would give a bandwidth of 116~GB/s. The correlate kernel slightly
outperforms this because its memory accesses are not purely loads. Thus, we
conclude that these kernels are bandwidth limited, but achieve essentially the
peak bandwidth feasible.

\enlargethispage*{1000pt}
For the two kernels that we have vectorized and parallelized, we
find that the parallelization is reasonably good but the performance of
vectorization much lower than one might expect. However, this is directly
attributable to bandwidth limitation of the kernels, which do achieve close to
the peak bandwidth for the architecture.
\vspace*{15pt}

\newpage

\vspace*{-40pt}
\subsubsection{Performance relative to theoretical peak}
\vspace*{-05pt}
\label{sec:perf-relat-theor}

We have designed our overall algorithm to be dominated by
the FFT, and the optimal FFT implementation to be the multi-threaded FFTW
library. Our benchmark above gave approximately 960~$\mu$s as the execution time
of a $2^{20}$ single-precision, out-of-place complex inverse FFT; if we use
$5N\log{N}$ as the number of floating point 
operations performed by the FFT, then this corresponds to a performance of
95~GFlops. For comparison, we also measure the floating point operations using
the Linux \texttt{perf-stat} tool.  That measurement indicated first that
99.999\% of the instructions retired were single-precision AVX instructions, so
the FFTW library code is extremely well vectorized.  The corresponding
performance was 91~GFlops, or 83\% of the $5N\log{N}$ estimate. Since there are
FFT algorithms with a floating point count as low $4N\log{N}$, this is
consistent with the library having chosen an FFT algorithm with lower floating
point cost.  With eight AVX capable cores that can retire as many as two AVX
instructions per clock cycle, the E5-2670 has a peak theoretical floating-point
rate of 333 GFlops; we therefore achieve 27\% of the peak flop rate.  For an
algorithm with the complex memory access pattern of the FFT, this is a not
unreasonable performance. Regardless, since we expect to be FFT limited we
should not expect higher performance from the \texttt{pycbc\_inspiral}
executable as a whole than this. 

The performance of \texttt{pycbc\_inspiral} depends on the quality of the
data. Throughout our benchmarking studies we have consistently followed three
different types of data: (i) data which is nearly Gaussian and stationary,
representing very good data quality (Type A); (ii) data containing a single,
loud transient glitch (Type B), and (iii) data which contains elevated levels of
non-Gaussian noise at low frequencies (Type C).  The last category is the
worst in terms of computational cost, as the $\chi^2$ test must be invoked
frequently and the cost is dominated by the computation of that signal-based
veto. In late initial LIGO science runs this level of data quality was
extremely rare, and should the first observing runs of Advanced LIGO behave
similarly, it is not expected to greatly impact the computational cost.  The
costs we have presented, however, are conservative, and simply average the
throughput of the three categories of data.

Measurement of the floating point performance of \texttt{pycbc\_inspiral} showed
31~GFlops for Type C data, 41~GFlops for loud data, and 44~GFlops for Type A
(clean)
data.  These correspond to fractions of peak theoretical performance of 9.3\%,
12.2\%, and 13.3\%. We therefore still have room for improvement, and discuss in
the next subsection profiling results and their implications that identify the
next priorities for further optimization.

%\todo{Josh working on this section based on 5316 templates/core throughput for
%fftw 2 x 1 x 8 thread on E5-2670}

\vspace*{-10pt}
\subsubsection{Comparison of measured numbers with theoretical FFT throughput}
\vspace*{-05pt}
\label{sec:measured}

Finally we assess the overall performance of \texttt{pycbc\_inspiral} through
profiling. Continuing with the same three categories of data, we present a
profile run of \texttt{pycbc\_inspiral} in Table~\ref{tab:callgraph} for Type A and Type C data, to illustrate the two
extremes, for each kernel costing more than 1\% of the overall runtime. From
this table, the largest difference we observe is that the 
$\chi^2$ veto is only 4.2\% of the execution time in the Type A data, but 44.7\%
of the time in the Type C data. This is the reason Type C data is so problematic:
in this example $\chi^2$ is calculated four times as often as it was for Type
A data.
Hence more thorough vectorization and parallelization of this kernel is our next
optimization priority.

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|r|r|}\hline
   \multirow{2}{*}{\textbf{Kernel}} & \multicolumn{2}{c|}{\textbf{Type A Data}}
    & \multicolumn{2}{|c|}{\textbf{Type C Data}} \\ \cline{2-5}
    & Absolute time (s) & Percentage & Absolute time (s) & Percentage \\ \hline
    FFT & 1304 & 60.4 & 1159 & 32.3 \\ \hline
    correlate & 332 & 13.9 & 300 & 8.4 \\ \hline
    template creation & 203 & 9.4 & 202 & 5.6 \\ \hline
    threshold \& cluster & 97 & 4.5 & 87 & 2.4 \\ \hline
    $\chi^2$ & 90 & 4.2 & 1601 & 44.7 \\ \hline
    data resampling & 35 & 1.6 & -- & $<$1 \\ \hline
    recording triggers & -- & $<$1 & 49 & 1.4 \\ \hline \hline
    \emph{Total runtime} & 2158 & 100 & 3583 & 100 \\ \hline
  \end{tabular}
  \caption{Profiling results for Type A and Type C data at a 4096~Hz sample
    rate on an E5-2670. This table summarized the data shown in detail in
Figures~\ref{fig:clean-callgraph} and \ref{fig:grumbly-callgraph} of Appendix\ref{a:lalapps}.}
  \label{tab:callgraph}
\end{table}

Since our goal is for the \texttt{pycbc\_inspiral} engine to be FFT limited, we also use the profile information above to
measure the average execution time per FFT
\textit{in situ} and compare that to the benchmarked performance for our
optimal FFT configurations as shown in Table\footnote{Note that the $2^{19}$
length FFT in Table~\ref{tab:fft} corresponds to a 2048~Hz sample rate, and a
$2^{20}$ length FFT to a 4096~Hz sample rate.}~\ref{tab:fft}. We present this in
Table~\ref{tab:insp_fft}. From these results we see that for the 2048~Hz sample
rate, the effective execution time of 516~$\mu$s is 84~$\mu$s longer than
benchmarked average FFT time of 432~$\mu$s, whereas for the 4096~Hz sample rate
the observed FFT time of 1470~$\mu$s is 370~$\mu$s greater than that obtained by
benchmarking the FFT in isolation. We can understand this if we recall that the
last-level (level 3) cache of the E5-2670 is 20~MB.  While the memory of an
out-of-place $2^{20}$ FFT fits inside this at 16~MB, the total memory required
for our matched-filter inner loop of correlation, FFT, and threshold and
clustering requires a total of 24~MB and does not fit in cache. Because the
different areas of memory comprising this 24~MB are accessed at widely separated
(in time) parts of this loop, hardware prefetching is unlikely to be able to
hide much of this latency.  We can validate this explanation by referring to the
2048~Hz sample rate results, where the total memory required by all of the
kernels in the matched filter is 12~MB which does fit in cache. And indeed we
see that the \textit{in situ} execution time of that FFT is much closer to the
isolated benchmark. As a further check, we have counted the number of last-level
cache misses of each sample rate, when analyzing the same data with the same
bank and number of segments. The 4096~Hz sample rate analysis incurs between 11
and 15 times (depending on data quality) as many cache misses as the 2048~Hz
analysis, even though both performed exactly the same number of matched
filters. 

\begin{table}
  \centering
  \begin{tabular}{|c|r|r|r|r|}\hline
   \textbf{Sample Rate} & \textbf{Type A Data}& \textbf{Type B Data} & 
   \textbf{Type C data} & \textbf{Average} \\ \hline
    2048~Hz & 517 & 518 & 512 & 516 \\ \hline
    4096~Hz & 1520 & 1530 & 1350 & 1470 \\ \hline
  \end{tabular}
  \caption{Effective execution time ($\mu$s) of FFT within
    \texttt{pycbc\_inspiral} on E5-2670 socket (FFTW, eight-threaded). }
  \label{tab:insp_fft}
\end{table}

We are investigating ways to alleviate this penalty, and discuss some of these
in the next subsection on future optimizations. Alternatively, it is not yet
decided on what hardware the various PyCBC searches will run, and should they do
so on hardware with sufficiently large cache the issue could be moot.

\vspace*{-10pt}
\subsubsection{Future CPU optimizations}
\vspace*{-05pt}

We are investigating a number of performance optimizations to more efficiently
implement the existing computational methods: vectorization and parallelization
of the template generation and $\chi^2$ veto, and bypassing the CPU cache for
loads of some memory, to mitigate the cache eviction causing the degraded
\textit{in situ} performance of the $2^{20}$ size FFT. The latter are in
principle possible using the streaming load operations that became available in
SSE~4.1, but also require the memory from which they read to be marked as
uncacheable, speculative write-combining (USWC) which is only possible through a
kernel module. Aside from these implementation optimizations, as briefly
mentioned in section~\ref{sec:MethodSpace}, we are also exploring alternative
scientific methods (such as hierarchical searches and pruned FFTs) that if
verified through simulations do not degrade sensitivity can provide potentially
large computational savings.

\vspace*{-10pt}
\section{Selecting optimal hardware solutions}
\vspace*{-05pt}
\label{sec:offline-trade-study}

One of the primary factors guiding the design of the new PyCBC framework was
to provide the ability to implement compute kernels on a variety of
architectures, including CPUs, GPUs, and the Intel Many Integrated Core
Architecture (MIC) co-processors. Over the past year, we have focused on
implementing the \textsc{Findchirp} algorithm on GPUs, as the NVIDIA CUDA FFT
library~\cite{cufft} implements an extremely efficient ``black box'' FFT that
scales very well to the large $2^{20}$ (and longer) complex FFTs used in the
offline CBC search. Furthermore, since matched filtering LIGO data
can be performed at single precision, we have investigated
inexpensive consumer-grade GPU cards as a possible computing platform for the
high-latency CBC search\footnote{NVIDIA artificially reduces the speed of
double precision arithmetic on the consumer-grade GPU units, but
single-precision arithmetic runs at full speed.}.  Section~\ref{sec:gpu-trade} describes the CUDA implementation of the
\textsc{findchirp} algorithm and the initial results of our GPU hardware trade
study. Finally Section~\ref{sec:cpu-trade} describes the results of our trade
study investigating the performance of the best CPU \texttt{pycbc\_inspiral}
implementation on different CPUs, including Intel Westmere, Sandybridge,
Ivybridge, and Haswell.

\vspace*{-10pt}
\subsection{PyCBC on Graphics Processing Units}
\vspace*{-05pt}
\label{sec:gpu-trade}

\begin{table}[!b]
\centering
{\small
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline 
{\bf GPU Card Type}  & \multicolumn{2}{c|}{Memory Bandwidth (GB/s)} & \multicolumn{2}{c|}{SP Performance (GFLOPS)} & FFT GFLOPS & Cost \\
                     & Theoretical & Measured & Theoretical & Measured & Measured &  \\\hline
GTX 580 & 192 & 170 & 1581 & 1553 & 444 & N/A \\
GTX 980 & 224 & 179 & 4612 & 4980 & 456 & \$555\\
GTX 970 & 224 & 155 & 3494 & 4025 & 357 & \$329 \\
GTX 750 Ti & 86 & 80 & 1306 & 1490 & 150 & \$139 \\
Tesla 2090 & 177 & 106 & 1331 & 1309 & 361 & N/A \\
Tesla K10 & 160 & 101 & 2290 & 2015 & 288 & \$2800 \\
Tesla K80 & 240 & 170 & 4350 & 3712 & 288 & \$5000 \\\hline
\end{tabular}
}
\caption{\label{tab:gpu-test-stand}
Theoretical and measured performance of the GPUs investigated in our trade
study. The theoretical performance for the consumer grade cards is
taken from the NVIDIA reference implementation of the GPU. Faster than
theoretical measured performance can be obtained if the consumer card
manufacturers (e.g.  PNY or EVGA) overclock their cards compared to the
reference implementation. Columns two and three compare the published
theoretical and measured bandwidth from the GPU global memory to the processor
(in GB/s).  Columns four and five compare the published theoretical
single-precision computational speed (in GFLOPS) for the reference
implementation of the GPU with the speed measured on our cards.  Column six
shows the computational speed of the cuFFT for large transforms, which is
memory bandwidth limited and column seven shows the (March 2015) cost of the
card. We use ORNL SHOC to measure the \emph{in situ} performance. Note that
the Tesla K10 and K80 GPU boards contain two independent GK104 and GK210 GPU
chips. The performance numbers quoted here are for a single GPU chip, and not
the board. The GTX 580 and Tesla M2090 are no longer in production. These
cards cost \$500 and \$2500, respectively, when purchased.
}
\end{table}

Our goal when implementing the GPU-enabled version of \textsc{pycbc\_inspiral}
is to execute as much computation on the GPU, with as little data passing
over the (slow) PCIe host interconnect as possible. Simply off-loading the
FFT to the GPU does not significantly speed up the code, due to the
rate-limiting step of moving the input and output vectors over the PCIe bus.
Fortunately, the \textsc{findchirp} algorithm lends itself well to 
performing all computations on the GPU, as the pre-conditioned input data segments can be stored in global GPU
memory and then processed through many templates that are generated on
the GPU. Our GPU implementation therefore implements as CUDA-native kernels
\emph{both} the compute-intensive steps of the algorithm (correlate, FFT, and
time-frequency signal-based veto) \emph{and} the relatively light-weight steps
(template generation and threshold/cluster), ensuring that only very minimal
PCIe bandwidth is required to initially stage the data to GPU memory and pass
triggers back to host memory. 


For large regions of parameter space, template generation can be expressed as
an analytic polynomial, which we have implemented as a straightforward
element-wise GPU kernel. Work is ongoing on extending template generation to
other waveform approximants that are more appropriate for modeling higher mass
BBH systems. As the correlate kernel is a point-wise complex multiply and
conjugate, the GPU implementation is also straightforward.  We make use of
NVIDIA's proprietary cuFFT library to perform inverse FFTs.  This library
factors the FFT into multiple kernel calls based on the size of the FFT and
the GPU hardware capability. On a Tesla K10, using CUDA 6.5, FFT sizes between
$2^{20}$ and $2^{23}$ all factor into three kernels calls. As the FFT is
memory bandwidth bound, it is clear that for these range of sizes the FFT
throughput will scale linearly with vector length.  Thresholding and
clustering is divided into two kernels. The first performs both thresholding
and local peak finding on small fixed window sizes. The kernel window sizes
are smaller than the scientifically chosen clustering window. This exposes an
additional parallelism. A second, very short-running kernel that executes a
single block, is used to perform final cleanup and boundary condition
checking. Following this kernel, we dump triggers back to the host, which due
to the on-GPU clustering is guaranteed to be O($10^{-3}$) the size of the data
vectors in the worst case, and on average much less.  Finally, we have also
implemented our time-frequency signal consistency test as a set of GPU kernels
where each is designed to handle a different number of triggers. This is
implemented using a standard parallel reduction sum operation. 

\vspace*{-10pt}
\subsubsection{GPU Benchmarking Results}
\vspace*{-05pt}

Similar to the CPU implementation, the 3 kernels that dominate the inner loop
of the matched-filter (correlate, FFT, and thresholding) are all memory
bandwidth bound. Therefore both memory bandwidth and floating point performance
 are considerations when selecting the optimal GPU hardware.
Table~\ref{tab:gpu-test-stand} shows the GPU hardware that we have procured to
test the CUDA implementation of the \textsc{findchirp}
algorithm\footnote{Unfortunately, our Tesla K80 proved to be unstable in the
Super Micro test chassis that we are using, and so we have not been able to
measure sustained performance on this card.}. We have benchmarked
\texttt{pycbc\_inspiral} on these GPUs, with the results shown in 
Table~\ref{tab:gpu-results} in templates per GPU chip (note that in practice,
the overall throughput of the K10 is a factor of two faster than the numbers
quoted here, as it contains two GPU chips per PCIe board).

Using templates per dollar as the performance benchmark, the best performing
GPU is the GTX 750 Ti, with an average throughput of 790 templates per dollar.
This is the cheapest of the consumer-grade GPUs that we have tested. This card
also has the advantage that it is powered by the PCIe bus (no additional 6- or
8-pin PCIe power connectors are needed) and it can easily be converted to a 1U
profile.  For comparison, the Intel E5-2670 (which currently retails for
\$1365) has an average throughput of 42,500 templates per socket or 31
templates per dollar.  The best performing CPU we have tested using the cost
metric is the Intel E3-1220-v3 which has a throughput of 28,340 templates per
socket at a cost of \$205, or 138 templates per dollar. It is important to
note that this metric oversimplifies the comparison between CPUs and GPUs, as
GPUs require a CPU-based host system. The true cost metric should take this
into account. However, the consumer-grade cards are a very promising avenue
for exploration for co-processors in CPU-based systems or in custom GPU
clusters. We can also use the CUDA kernels in \texttt{pycbc\_inspiral} to take
advantage of XSEDE resources that provision HPC-grade GPUs.

Consumer-grade GPU units do not have the memory error checking and correction
(ECC) provided in the NVIDIA's HPC  GPU line\footnote{We note that Tesla ECC
is implements in software, which further slows down the throughput of the
cards from theoretical peak.} and so a concern when constructing
consumer-grade GPU clusters is reliability of the compute units. 
%Our first
%large-scale system containing consumer-grade GPUs is the Syracuse University
%Gravtation and Relativity (SUGAR) cluster. A CPU based system was constructed
%using NSF MRI funding\footnote{NSF award PHY-1040231.} using Westmere CPUs,
%with space for a GPU card in each of 172 hosts. Syracuse University ITS
%provided funding to providing a GTX580 GPU card in each of the host systems.
%This cluster was used to develop the CUDA kernels and perform large-scale
%testing of the CBC searches on consumer-grade GPUs. 
To investigate the reliability of the consumer grade cards, we run the ORNL
SHOC \texttt{stability} code~\cite{shoc}. This program generates a series of
random numbers that and then repeatedly computes the forward and reverse FFT
of the input vector.  On each iteration, the code checks that the calculated
output vector agrees with the original input vector and reports errors. A
sustained two-day test with 13 GTX750 Ti cards showed no errors. This
is more encouraging than our original tests with GTX580 cards, which showed a
relatively high rate of errors\footnote{However, since the GTX580 was a factor of five
less expensive than the equivalent Tesla M2090, it was still more
cost-effective, even at the cost of running all computations twice to check for errors.}.  We
attribute this difference to the fact that the GTX 750 Ti runs at lower clock
speeds and lower power than the GTX580. Testing of the GeForce 900 series
of cards to see if they also show this level of reliability is ongoing. Given
these considerations, consumer grade GPU cards can yield a very cost-effective 
platform for the offline CBC search. 
%In the next two sections, we describe the
%current CUDA implementation of the \textsc{findchirp} algorithm and review
%possible future optimizations.


\begin{table}[!t]
\centering
{\small
\begin{tabular}{|l|c|c|c|c|}
\hline 
{\bf GPU Card Type}  & \multicolumn{3}{c|}{Search throughput (templates / GPU board)} & Average \\
                     & Type A Data & Type B Data & Type C Data & throughput \\\hline
GTX 580 & 216,100 & 210,000 & 151,700 & 192,600 \\
GTX 980 & 221,000 & 213,800 & 154,900 & 196,600 \\
GTX 970 & 199,300 & 194,000 & 144,800 & 179,400 \\
GTX 750 Ti & 120,700 & 116,600 & 92,000 & 109,800 \\
Tesla 2090 & 154,800 & 149,000 & 117,200 & 140,300 \\
Tesla K10 & 133,400 & 126,600 & 95,200 & 118,400 \\\hline
\end{tabular}
}
\caption{\label{tab:gpu-results}
Computational throughput of \texttt{pycbc\_inspiral} running the CUDA GPU
compute kernels on consumer and HPC-grade GPUs for the three types of data
used for benchmarking. The input data sample rate is 4096~Hz and the code
processed 15 data segments of length 256~s through each template (for
comparison with the CPU numbers). Two independent \texttt{pycbc\_inspiral}
processes schedule CUDA kernels on a single GPU chip to maximize the
throughput.  For comparison, the throughput of an eight core E5-2670 socket is
51,120 templates/socket for Type A data and 29,400 templates/socket for Type C
data.
}
\end{table}

\vspace*{-10pt}
\subsubsection{Optimization of the GPU Implementation}
\vspace*{-05pt}

While our initial CUDA implementation of the the \textsc{findchirp} algorithm
is efficient in the sense that as much computation is performed on the GPU as
possible, we have identified several areas for future optimization. 
Since all of our input data is staged to the GPU, the rate limiting factor for
our current implementation is the memory bandwidth between the GPU's global
memory and the on-chip Level 2 cache and registers where threads access data
for computation.  Our primary goal in optimizing the GPU implementation has
been to reduce the number of memory transfers and maximize the use of the
GPU's floating point engine.  CUDA kernels operate on data in GPU global
memory and for each kernel call, data is transferred across the memory
bus\footnote{Typically DDR3 or GDDR5 depending on the model of GPU card.} from
GPU global memory to the registers of the processor cores and back to global
memory at the end of the kernel.  A basic performance analysis can be obtained by
counting the memory operations executed by the correlate, FFT, and threshold
kernels used in the \textsc{findchirp} loop:
\begin{equation}
\textrm{Correlate} (2 \textrm{in}+1 \textrm{out}) + \textrm{FFT} (3 \textrm{in}+3 \textrm{out}) + \textrm{threshold} (1 \textrm{in})  = 10 \ \textrm{memory transfers}
\end{equation}
With the release of CUDA 6.5, a new feature was added to the cuFFT
library that allows user defined callback functions for both the load of the
initial input vector and the store of the final output vector of the FFT.
This has the potential of allowing us to fuse computations from the correlate
and threshold steps into the FFT kernel, reducing the number of memory
transfers and increasing performance. Our first step towards optimizing our
CUDA implementation has been to investigate the use of callbacks.
%Callbacks have the potential to significantly improve the GPU performance by
%reducing the number of memory transfers, potentially to 6.

The current implementation of NVIDIA's cuFFT callback API allows
element-by-element functions to be easily applied, with no guarantee about the
relationship between nearby elements or order of operations within the kernel
itself. Because the callbacks cannot be compiled into the FFT kernels
themselves, and can handle only single elements, there is significant overhead
to their use that cannot be easily predicted without benchmarking.
Fig.~\ref{fig:callback} compares the relative execution time of the three
kernels that make up the inner loop of the matched-filter code under three
cases. The first case (left) uses the initial kernel implementations without
making use of the callback API. The second case (middle) fuses the correlate
kernel, without modification, into a load callback. We see that there is a
noticeable drop in the total execution time. The savings comes from the
removal of both a full vector length store and read operation. Note however,
that this is significantly less improvement than would expected from a naive
counting of the memory savings. The final case takes full advantage of the
known contiguous regions where the input vectors are zero, and where the
output vector does not produce valid results due to wrap-around corruption.
Callbacks appear to be a very promising avenue of optimization, and our
collaborators on the NVIDIA cuFFT team are interested in our application as a
use-case for developing the API further.

For certain kinds of commonly used waveform templates, in particular the
TaylorF2 approximant, the amplitude of the waveform is a simple power series.
This allows it to be precomputed, and instead of including it with the
template itself can be pre-multiplied into the segment of data to analyze.
Where this is possible, the remaining portion of the template can be expressed
in the form $e^{i\psi(f)}$. It is possible to trade floating point operations
for a savings in global memory reads by storing only the Fourier phase of the
template, $\psi(f)$, and recalculating the full $e^{i\psi(f)}$ within a load
callback of the FFT.  If the callback API can be extended to allow a
vectorized version of the store callback that operates on contiguous
elements, it may be possible to merge a portion of the peak finding algorithm
into the store callback, vastly decreasing the memory writes at the end of the
fused kernel.

%\begin{SCfigure}[][!t]
%\centering
%\includegraphics[width=0.6\textwidth]{callback.png}
%\caption{\label{fig:callback} 
%The relative performance of the kernels that make up the critical inner %loop of the 
%matched-filtering code. Shorter bars represent better performance. Left: The initial GPU kernel implementations without the
%use of cuFFT callbacks. Middle: Naive fusion of the correlate into a load callback.
%Right: Fusion of the correlate kernel into the load callback, where memory reads
%are avoided where the input is known to be zeros, and output writes are avoided
%where it is known to be corrupted by wrap-around effects. It is not currently
%possible to fuse the threshold kernel into the FFT, however we are working
%with NVIDIA to make the necessary changes to the cuFFT callback API to further
%optimize the code.
%}
%\end{SCfigure}

More optimal use of the available memory bandwidth can also be achieved by
reducing the amount of data sent over the memory bus. We are investigating the
possibility of storing the output SNR time and input template phase as
half-precision (FP16) numbers to reduce memory bandwidth. We have also
discussed with NVIDIA the possibility of adding callbacks to the intermediate
steps of the cuFFT implementation (since our $2^{20}$ point FFTs are
implemented by three kernel calls in cuFFT) that would allow us to use FP16
precision between each FFT radix. Performing the FFT operations
in FP32 and storing the intermediate products in FP16 may be possible. We are beginning
a study to determine if this model could meet our accuracy requirements.

Finally, we are investigating the optimal GPU/CPU ratio for systems and
parellization between the host CPU and GPU kernel execution. As GPU kernel
launches are asynchronous compared to host execution, it is possible to hide
trivial serial operations that occur within the host code. The exception is
where triggers are offloaded from the GPU onto the CPU, which is a blocking
operation. Host execution does not proceed until the GPU queue is drained.
When the data is synchronized there is a noticeable delay before new GPU
kernels are executing. This can be minimized by executing multiple host
processes that submit work to the same GPU, and by batching additional work
together to amortize the device offload latency. We have shown that two
processes running on the same CPU launching kernels to a single GPU makes more
efficient use of the GPU resources; tests to find the optimal ratio are
ongoing.



